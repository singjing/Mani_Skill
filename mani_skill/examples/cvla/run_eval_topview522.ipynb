{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05af6d42-c4a4-4f05-ae7d-7f1d10239b8f",
   "metadata": {},
   "source": [
    "# Update by the top view imaage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "716f20ca-0ad8-45cc-8c5c-d27b30eabc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main entry point for running the CVLA environment. See readme for details.\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import traceback\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Annotated, Union\n",
    "\n",
    "import tyro\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "import sapien\n",
    "\n",
    "from mani_skill.utils.structs import Pose\n",
    "from mani_skill.utils.wrappers import RecordEpisode\n",
    "import mani_skill.examples.cvla.cvla_env  # do import to register env, not used otherwise\n",
    "from mani_skill.examples.cvla.utils_trajectory import generate_curve_torch, DummyCamera\n",
    "from mani_skill.examples.cvla.utils_traj_tokens import getActionEncInstance, to_prefix_suffix\n",
    "from mani_skill.examples.cvla.utils_record import apply_check_object_pixels_obs\n",
    "from mani_skill.examples.cvla.utils_record import downcast_seg_array\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "\n",
    "RAND_MAX = 2**32 - 1\n",
    "SAVE_FREQ = 1  # save after every reset\n",
    "RESET_HARD = 10  # re-start environment after every n steps\n",
    "SAVE_VIDEO = False  # save videos\n",
    "# minimum percentage of image that must be object, set to None to disable checking\n",
    "MIN_OBJ_VISIBLE_PERCENT = 0.5\n",
    "\n",
    "\n",
    "def getMotionPlanner(env):\n",
    "    if env.unwrapped.robot_uids in (\"panda\", \"panda_wristcam\"):\n",
    "        from mani_skill.examples.motionplanning.panda.motionplanner import \\\n",
    "            PandaArmMotionPlanningSolver as RobotArmMotionPlanningSolver\n",
    "    elif env.unwrapped.robot_uids == \"fetch\":\n",
    "        from mani_skill.examples.motionplanning.fetch.motionplanner import \\\n",
    "            FetchArmMotionPlanningSolver as RobotArmMotionPlanningSolver\n",
    "    else:\n",
    "        raise ValueError(f\"no motion planner for {env.unwrapped.robot_uids}\")\n",
    "    return RobotArmMotionPlanningSolver\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    env_id: Annotated[str, tyro.conf.arg(aliases=[\"-e\"])] = \"CvlaMove-v1\"\n",
    "    \"\"\"The environment ID of the task you want to simulate\"\"\"\n",
    "\n",
    "    obs_mode: Annotated[str, tyro.conf.arg(aliases=[\"-o\"])] = \"rgb+depth+segmentation\"\n",
    "    \"\"\"Observation mode\"\"\"\n",
    "\n",
    "    sim_backend: Annotated[str, tyro.conf.arg(aliases=[\"-b\"])] = \"auto\"\n",
    "    \"\"\"Which simulation backend to use. Can be 'auto', 'cpu', 'gpu'\"\"\"\n",
    "\n",
    "    reward_mode: Optional[str] = None\n",
    "    \"\"\"Reward mode\"\"\"\n",
    "\n",
    "    num_envs: int = 1\n",
    "    \"\"\"Number of environments to run.\"\"\"\n",
    "\n",
    "    control_mode: Annotated[Optional[str], tyro.conf.arg(aliases=[\"-c\"])] = \"pd_joint_pos\"\n",
    "    \"\"\"Control mode\"\"\"\n",
    "\n",
    "    render_mode: str = \"rgb_array\"\n",
    "    \"\"\"Render mode\"\"\"\n",
    "\n",
    "    shader: str = \"default\"\n",
    "    \"\"\"Change shader used for all cameras in the environment for rendering. Default is 'minimal' which is very fast. Can also be 'rt' for ray tracing and generating photo-realistic renders. Can also be 'rt-fast' for a faster but lower quality ray-traced renderer\"\"\"\n",
    "\n",
    "    record_dir: Optional[str] = None\n",
    "    \"\"\"Directory to save recordings\"\"\"\n",
    "\n",
    "    pause: Annotated[bool, tyro.conf.arg(aliases=[\"-p\"])] = False\n",
    "    \"\"\"If using human render mode, auto pauses the simulation upon loading\"\"\"\n",
    "\n",
    "    quiet: bool = False\n",
    "    \"\"\"Disable verbose output.\"\"\"\n",
    "\n",
    "    seed: Annotated[Optional[Union[int, List[int], str]], tyro.conf.arg(aliases=[\"-s\"])] = None\n",
    "    \"\"\"Seed(s) for random actions and simulator. Can be a single integer or a list of integers. Default is None (no seeds)\"\"\"\n",
    "\n",
    "    run_mode: Annotated[Optional[str], tyro.conf.arg(aliases=[\"-m\"])] = \"script\"\n",
    "    \"\"\"Run mode, options are script, interactive, first\"\"\"\n",
    "\n",
    "    robot_uids: Annotated[Optional[str], tyro.conf.arg(aliases=[\"-r\"])] = \"panda\"\n",
    "    \"\"\"Robots, options are: panda, panda_wristcam, xarm6_robotiq, floating_inspire_hand_right\"\"\"\n",
    "\n",
    "    scene_dataset: Annotated[Optional[str], tyro.conf.arg(aliases=[\"-sd\"])] = \"Table\"\n",
    "    \"\"\"Scene datasets: options are: Table, ProcTHOR\"\"\"\n",
    "\n",
    "    scene_options: Annotated[Optional[str], tyro.conf.arg(aliases=[\"-so\"])] = \"fixed\"\n",
    "    \"\"\"Randomize the scene\"\"\"\n",
    "\n",
    "    object_dataset: Annotated[Optional[str], tyro.conf.arg(aliases=[\"-od\"])] = \"clevr\"\n",
    "    \"\"\"Dataset from which we sample objects, options are: clevr, ycb, objaverse\"\"\"\n",
    "\n",
    "    camera_views: Annotated[Optional[str], tyro.conf.arg(aliases=[\"-cv\"])] = \"random_side\"\n",
    "    \"\"\"Dataset from which we sample objects\"\"\"\n",
    "\n",
    "    action_encoder: Annotated[Optional[str], tyro.conf.arg(aliases=[\"-ae\"])] = \"xyzrotvec-cam-1024xy\"\n",
    "    \"\"\"Action encoding\"\"\"\n",
    "\n",
    "    N_samples: Annotated[Optional[int], tyro.conf.arg(aliases=[\"-N\"])] = 50\n",
    "    \"\"\"Number of samples\"\"\"\n",
    "\n",
    "\n",
    "def reset_random(args, orig_seeds):\n",
    "    if orig_seeds is None:\n",
    "        seed = random.randrange(RAND_MAX)\n",
    "    elif isinstance(orig_seeds, list):\n",
    "        seed = orig_seeds.pop()\n",
    "    elif isinstance(orig_seeds, int):\n",
    "        seed = orig_seeds\n",
    "    else:\n",
    "        raise ValueError\n",
    "    args.seed = [seed]\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def iterate_env(args: Args, vis=True, model=None):\n",
    "    np.set_printoptions(suppress=True, precision=3)\n",
    "    verbose = not args.quiet\n",
    "    parallel_in_single_scene = args.render_mode == \"human\"\n",
    "    if args.render_mode == \"human\" and args.obs_mode in [\"sensor_data\", \"rgb\", \"rgbd\", \"depth\", \"point_cloud\"]:\n",
    "        print(\"Disabling parallel single scene/GUI render as observation mode is a visual one. Change observation mode to state or state_dict to see a parallel env render\")\n",
    "        parallel_in_single_scene = False\n",
    "    if args.render_mode == \"human\" and args.num_envs == 1:\n",
    "        parallel_in_single_scene = False\n",
    "\n",
    "    # define make env as a function to enable hard resets\n",
    "    def make_env():\n",
    "        env = gym.make(\n",
    "            args.env_id,\n",
    "            obs_mode=args.obs_mode,\n",
    "            reward_mode=args.reward_mode,\n",
    "            control_mode=args.control_mode,\n",
    "            render_mode=args.render_mode,\n",
    "            sensor_configs=dict(shader_pack=args.shader),\n",
    "            human_render_camera_configs=dict(shader_pack=args.shader),\n",
    "            viewer_camera_configs=dict(shader_pack=args.shader),\n",
    "            num_envs=args.num_envs,\n",
    "            sim_backend=args.sim_backend,\n",
    "            parallel_in_single_scene=parallel_in_single_scene,\n",
    "            robot_uids=args.robot_uids,\n",
    "            scene_dataset=args.scene_dataset,\n",
    "            object_dataset=args.object_dataset,\n",
    "            camera_views=args.camera_views,\n",
    "            scene_options=args.scene_options,\n",
    "            # camera_cfgs={\"use_stereo_depth\": True, },\n",
    "            # **args.env_kwargs\n",
    "        )\n",
    "        if args.record_dir:\n",
    "            env = RecordEpisode(env, args.record_dir, info_on_video=False,\n",
    "                                save_trajectory=True, max_steps_per_video=env._max_episode_steps,\n",
    "                                save_on_reset=SAVE_FREQ == 1,\n",
    "                                record_env_state=True)\n",
    "        return env\n",
    "\n",
    "    env = make_env()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Observation space\", env.observation_space)\n",
    "        print(\"Action space\", env.action_space)\n",
    "        print(\"Control mode\", env.unwrapped.control_mode)\n",
    "        print(\"Reward mode\", env.unwrapped.reward_mode)\n",
    "        print(\"Render mode\", args.render_mode)\n",
    "        print(\"Obs mode\", args.obs_mode)\n",
    "\n",
    "    filter_visible = True\n",
    "    action_encoder = getActionEncInstance(args.action_encoder)\n",
    "    enc_func, dec_func = action_encoder.encode_trajectory, action_encoder.decode_trajectory\n",
    "\n",
    "    print(\"action encoder\", args.action_encoder)\n",
    "    print(\"filter visible objects\", filter_visible)\n",
    "\n",
    "    orig_seeds = args.seed\n",
    "    N_valid_samples = 0\n",
    "    max_attempts = 10**6\n",
    "    for i in range(max_attempts):\n",
    "        reset_random(args, orig_seeds)\n",
    "        assert isinstance(args.seed, list)\n",
    "\n",
    "        if i != 0 and i % RESET_HARD == 0:\n",
    "            del env\n",
    "            env = make_env()\n",
    "        try:\n",
    "            obs, _ = env.reset(seed=args.seed[0], options=dict(reconfigure=True))\n",
    "        except Exception as e:  # Catch all exceptions, including AssertionError\n",
    "            print(f\"Encountered error {e.__class__.__name__} at seed {args.seed[0]} while resetting env. Skipping this iteration.\")\n",
    "            print(e)\n",
    "            traceback.print_exc()  # Prints the full traceback\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "\n",
    "        if MIN_OBJ_VISIBLE_PERCENT is None:\n",
    "            obj_are_vis = True\n",
    "        else:\n",
    "            obj_are_vis = apply_check_object_pixels_obs(obs, env, N_percent=MIN_OBJ_VISIBLE_PERCENT)\n",
    "        if not obj_are_vis:\n",
    "            print(\"Warning: object not visible, skipping sample\")\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "\n",
    "        # Note: when using RecordEpisode this will create 20x the number of saved frames\n",
    "        # so 75GB -> 1.5 TB, which is no good.\n",
    "        # Let the objects settle (!)\n",
    "        # for _ in range(20):\n",
    "        #    _ = env.step(obs[\"agent\"][\"qpos\"][..., :8])\n",
    "\n",
    "        if args.seed is not None:\n",
    "            env.action_space.seed(args.seed[0])\n",
    "        if vis and args.render_mode is not None:\n",
    "            viewer = env.render()\n",
    "            if isinstance(viewer, sapien.utils.Viewer):\n",
    "                viewer.paused = args.pause\n",
    "            env.render()\n",
    "        else:\n",
    "            env.render()\n",
    "\n",
    "        # Not parrelized\n",
    "        # env_idx = 0\n",
    "\n",
    "        # -----\n",
    "        # Warning, taking an image form obs/rendering it results in different calibrations!\n",
    "        # e.g. images = env.base_env.scene.get_human_render_camera_images('render_camera')\n",
    "        # -----\n",
    "        obj_start = Pose(obs[\"extra\"][\"obj_start\"].clone().detach())\n",
    "        obj_end = Pose(obs[\"extra\"][\"obj_end\"].clone().detach())\n",
    "        grasp_pose = Pose(obs[\"extra\"][\"grasp_pose\"].clone().detach())\n",
    "        tcp_pose = Pose(obs[\"extra\"][\"tcp_pose\"].clone().detach())\n",
    "        robot_pose = Pose(obs[\"extra\"][\"robot_pose\"].clone().detach())\n",
    "\n",
    "        try:\n",
    "            camera_intrinsic = obs[\"sensor_param\"][\"render_camera\"][\"intrinsic_cv\"].clone().detach()\n",
    "            camera_extrinsic = obs[\"sensor_param\"][\"render_camera\"][\"extrinsic_cv\"].clone().detach()\n",
    "            image_before = obs[\"sensor_data\"][\"render_camera\"][\"rgb\"][0].clone().detach()\n",
    "            depth = obs[\"sensor_data\"][\"render_camera\"][\"depth\"][0].clone().detach()\n",
    "            width, height, _ = image_before.shape\n",
    "            camera = DummyCamera(camera_intrinsic, camera_extrinsic, width, height)\n",
    "            # add depth to image_before\n",
    "            image_before = (depth, image_before)\n",
    "        except KeyError:\n",
    "            image_before = None\n",
    "            camera = env.base_env.scene.human_render_cameras['render_camera'].camera\n",
    "\n",
    "        action_text = env.unwrapped.get_obs_scene()[\"text\"]\n",
    "        assert isinstance(action_text, str) and action_text not in (None, \"\"), f\"action_text: {action_text}\"\n",
    "\n",
    "        prefix, token_str, curve_3d, orns_3d, info = to_prefix_suffix(obj_start, obj_end,\n",
    "                                                                      camera, grasp_pose, tcp_pose,\n",
    "                                                                      action_text, enc_func, robot_pose=robot_pose)\n",
    "\n",
    "        json_dict = dict(prefix=prefix, suffix=token_str,\n",
    "                         action_text=action_text,\n",
    "                         camera_extrinsic=camera.get_extrinsic_matrix().detach().numpy().tolist(),\n",
    "                         camera_intrinsic=camera.get_intrinsic_matrix().detach().numpy().tolist(),\n",
    "                         obj_start_pose=obj_start.raw_pose.detach().numpy().tolist(),\n",
    "                         obj_end_pose=obj_end.raw_pose.detach().numpy().tolist(),\n",
    "                         robot_pose=robot_pose.raw_pose.detach().numpy().tolist(),\n",
    "                         tcp_start_pose=tcp_pose.raw_pose.detach().numpy().tolist(),\n",
    "                         grasp_pose=grasp_pose.raw_pose.detach().numpy().tolist(),\n",
    "                         info=info,\n",
    "                         seed=args.seed[0],\n",
    "                         iter_reached=i,\n",
    "                         )\n",
    "\n",
    "        encode_decode_trajectory = True\n",
    "        if encode_decode_trajectory:\n",
    "            curve_3d_est, orns_3d_est = dec_func(token_str, camera, robot_pose=robot_pose)\n",
    "            curve_3d = curve_3d_est  # set the unparsed trajectory one used for policy\n",
    "            orns_3d = orns_3d_est\n",
    "        def get_pose_of_new_predicts(image,prefix,model):\n",
    "            pose_list = []\n",
    "            return pose_list\n",
    "\n",
    "        \n",
    "        # Evaluate the trajectory\n",
    "        if args.run_mode == \"script\" or model:\n",
    "            assert args.control_mode == \"pd_joint_pos\"\n",
    "            if verbose and info[\"didclip_traj\"]:\n",
    "                print(\"Warning refered object out of camera view.\")\n",
    "\n",
    "            if model:\n",
    "                _, _, _, token_pred = model.make_predictions(image_before, prefix)\n",
    "                json_dict[\"prediction\"] = token_pred\n",
    "                if token_pred == \"\" or token_pred is None:\n",
    "                    print(\"Warning: empty prediction, failing\")\n",
    "                    json_dict[\"reward\"] = 0\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    yield image_before, json_dict, args.seed[0]\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    curve_3d_pred, orns_3d_pred = dec_func(token_pred, camera=camera, robot_pose=robot_pose)\n",
    "                    curve_3d = curve_3d_pred  # set the unparsed trajectory one used for policy\n",
    "                    orns_3d = orns_3d_pred\n",
    "                # TODO(max): this should only catch value errors\n",
    "                except:\n",
    "                    print(\"Warning: exception during decoding tokens, failing\", token_pred)\n",
    "                    json_dict[\"reward\"] = 0\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    yield image_before, json_dict, args.seed[0]\n",
    "                    continue\n",
    "\n",
    "            # start and stop poses\n",
    "            if curve_3d.shape[1] != 2 or orns_3d.shape[1] != 2:\n",
    "                print(\"Warning: Model decoded something that is not a valid trajectory\")\n",
    "                json_dict[\"reward\"] = 0.0\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                yield image_before, json_dict, args.seed[0]\n",
    "                N_valid_samples += 1\n",
    "                continue\n",
    "\n",
    "            # convert two keypoints into motion sequence\n",
    "            _, curve_3d_i = generate_curve_torch(curve_3d[:, 0], curve_3d[:, -1], num_points=3)\n",
    "            grasp_pose = Pose.create_from_pq(p=curve_3d[:, 0], q=orns_3d[:, 0])\n",
    "            reach_pose = grasp_pose * sapien.Pose([0, 0, -0.10])  # Go above the object before grasping\n",
    "            lift_pose = Pose.create_from_pq(p=curve_3d_i[:, 1], q=orns_3d[:, 1])\n",
    "            align_pose = Pose.create_from_pq(p=curve_3d_i[:, 2], q=orns_3d[:, 1])\n",
    "            pre_align_pose = align_pose * sapien.Pose([0, 0, -0.10])  # Go above before dropping\n",
    "\n",
    "            # execute motion sequence using IK solver\n",
    "            RobotArmMotionPlanningSolver = getMotionPlanner(env)\n",
    "            planner = RobotArmMotionPlanningSolver(\n",
    "                env,\n",
    "                debug=False,\n",
    "                vis=vis,\n",
    "                base_pose=env.unwrapped.agent.robot.pose,\n",
    "                visualize_target_grasp_pose=vis,\n",
    "                print_env_info=False,\n",
    "            )\n",
    "            planner.move_to_pose_with_screw(reach_pose)\n",
    "            #get the current observation from top\n",
    "            obs1 = env.base_env.get_obs() \n",
    "            image_top = obs1[\"sensor_data\"][\"hand_camera\"][\"rgb\"][0].clone().detach()\n",
    "            \n",
    "            '''\n",
    "            image_top = image_top.cpu().numpy()\n",
    "            plt.imshow(image_top)\n",
    "            plt.axis('off')  # Turn off axis numbers/labels\n",
    "            plt.show()\n",
    "            '''\n",
    "            print(type(image_top))#.shape)\n",
    "            print(image_top.shape)\n",
    "            print(type(image_before))#.shape)\n",
    "            print(image_before[1].shape)\n",
    "            if model:\n",
    "                _, _, _, token_pred = model.make_predictions(image_top, prefix)\n",
    "                json_dict[\"prediction\"] = token_pred\n",
    "                if token_pred == \"\" or token_pred is None:\n",
    "                    print(\"Warning: empty prediction, failing\")\n",
    "                    json_dict[\"reward\"] = 0\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    yield image_before, json_dict, args.seed[0]\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    curve_3d_pred, orns_3d_pred = dec_func(token_pred, camera=camera, robot_pose=robot_pose)\n",
    "                    curve_3d = curve_3d_pred  # set the unparsed trajectory one used for policy\n",
    "                    orns_3d = orns_3d_pred\n",
    "                # TODO(max): this should only catch value errors\n",
    "                except:\n",
    "                    print(\"Warning: exception during decoding tokens, failing\", token_pred)\n",
    "                    json_dict[\"reward\"] = 0\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    yield image_before, json_dict, args.seed[0]\n",
    "                    continue\n",
    "            # start and stop poses\n",
    "            if curve_3d.shape[1] != 2 or orns_3d.shape[1] != 2:\n",
    "                print(\"Warning: Model decoded something that is not a valid trajectory\")\n",
    "                json_dict[\"reward\"] = 0.0\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                yield image_before, json_dict, args.seed[0]\n",
    "                N_valid_samples += 1\n",
    "                continue\n",
    "\n",
    "            # convert two keypoints into motion sequence\n",
    "            _, curve_3d_i = generate_curve_torch(curve_3d[:, 0], curve_3d[:, -1], num_points=3)\n",
    "            print(f\"origin grasp pos:{grasp_pose}\")\n",
    "            grasp_pose = Pose.create_from_pq(p=curve_3d[:, 0], q=orns_3d[:, 0])\n",
    "            print(f\"Updated grasp pos:{grasp_pose}\")\n",
    "            #reach_pose = grasp_pose * sapien.Pose([0, 0, -0.10])  # Go above the object before grasping\n",
    "            #lift_pose = Pose.create_from_pq(p=curve_3d_i[:, 1], q=orns_3d[:, 1])\n",
    "            #align_pose = Pose.create_from_pq(p=curve_3d_i[:, 2], q=orns_3d[:, 1])\n",
    "            #pre_align_pose = align_pose * sapien.Pose([0, 0, -0.10])  # Go above before dropping\n",
    "\n",
    "            \n",
    "            planner.move_to_pose_with_screw(grasp_pose)\n",
    "            # run_interactive(env)\n",
    "            planner.close_gripper()\n",
    "            planner.move_to_pose_with_screw(lift_pose)\n",
    "            planner.move_to_pose_with_screw(pre_align_pose)\n",
    "            planner.move_to_pose_with_screw(align_pose)\n",
    "            # run_interactive(env)\n",
    "            planner.open_gripper()\n",
    "            final_reward = env.unwrapped.eval_reward()[0]\n",
    "            planner.close()\n",
    "            json_dict[\"reward\"] = float(final_reward)\n",
    "            if verbose:\n",
    "                print(f\"reward {final_reward:0.2f} seed\", args.seed[0])\n",
    "\n",
    "        elif args.run_mode == \"interactive\":\n",
    "            run_interactive(env)\n",
    "        elif args.run_mode == \"first\":\n",
    "            # only render first frame\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        if args.record_dir:\n",
    "            # if i % SAVE_FREQ == 0:\n",
    "            # keep the transition from reset (which does not have an action)\n",
    "\n",
    "            downcast_seg_array(env)\n",
    "            env.flush_trajectory(save=True, ignore_empty_transition=False)\n",
    "            # to skip saving do: env.flush_trajectory(save=False)\n",
    "\n",
    "            if SAVE_VIDEO:\n",
    "                video_name = f\"CLEVR_{str(args.seed[0]).zfill(10)}\"\n",
    "                env.flush_video(name=video_name, save=True)\n",
    "\n",
    "        del obs\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        yield image_before, json_dict, args.seed[0]\n",
    "\n",
    "        N_valid_samples += 1\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "def run_interactive(env):\n",
    "    env.print_sim_details()\n",
    "    print(\"Entering do nothing loop: Ctrl-C to continue\")\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(.1)\n",
    "            env.base_env.render_human()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nCtrl+C detected, continuing.\")\n",
    "\n",
    "\n",
    "def run_iteration(parsed_args, N_samples, process_num=None, progress_bar=None):\n",
    "    \"\"\"Runs the environment iteration in a separate process.\"\"\"\n",
    "    env_iter = iterate_env(parsed_args, vis=False)\n",
    "    for _ in range(N_samples):\n",
    "        _ = next(env_iter)\n",
    "        if progress_bar is not None:\n",
    "            progress_bar.value += 1\n",
    "\n",
    "\n",
    "def save_multiproces(parsed_args, N_samples, N_processes=10):\n",
    "    from mani_skill.examples.cvla.utils_record import check_no_uncommitted_changes, get_git_commit_hash\n",
    "    parsed_args.run_mode = \"first\"\n",
    "    dataset_path = Path(parsed_args.record_dir)\n",
    "    os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "    # save command line arguments in nice format\n",
    "    if N_samples > 100:\n",
    "        check_no_uncommitted_changes()\n",
    "    commit_hash = get_git_commit_hash()\n",
    "    with open(dataset_path / \"args.txt\", \"w\") as f:\n",
    "        f.write(f\"git_commit: {commit_hash}\\n\")\n",
    "        for arg in vars(parsed_args):\n",
    "            f.write(f\"{arg}: {getattr(parsed_args, arg)}\\n\")\n",
    "\n",
    "    # set random seeds, be careful to not copy same seeds between processes\n",
    "    if N_processes > 1:\n",
    "        assert parsed_args.seed is None\n",
    "    if isinstance(parsed_args.seed, int):\n",
    "        assert N_processes == 1\n",
    "        rng = np.random.default_rng(parsed_args.seed)\n",
    "        parsed_args.seed = rng.integers(0, RAND_MAX, N_samples).tolist()\n",
    "\n",
    "    # don't multiprocess\n",
    "    if N_processes == 1:\n",
    "        # don't set N_samples in iterate_env, so that e.g. re-generate can work for visibility\n",
    "        env_iter = iterate_env(parsed_args, vis=False)\n",
    "        for _ in tqdm(range(N_samples)):\n",
    "            try:\n",
    "                _ = next(env_iter)\n",
    "            except StopIteration:\n",
    "                break\n",
    "    else:\n",
    "        samples_per_process = N_samples // N_processes\n",
    "        progress_bar = multiprocessing.Value(\"i\", 0)\n",
    "\n",
    "        tasks = []\n",
    "        for p_num in range(N_processes):\n",
    "            dataset_path_p = Path(dataset_path) / f\"p{p_num}\"\n",
    "            os.makedirs(dataset_path_p, exist_ok=True)\n",
    "            args_copy = deepcopy(parsed_args)\n",
    "            args_copy.record_dir = dataset_path_p\n",
    "            p = multiprocessing.Process(target=run_iteration, args=(args_copy, samples_per_process, p_num, progress_bar), name=f\"Worker-{p_num+1}\")\n",
    "            tasks.append(p)\n",
    "            p.start()\n",
    "            time.sleep(1.1)  # Give some time for processes to start\n",
    "\n",
    "        # Display tqdm progress in the main process\n",
    "        with tqdm(total=N_samples, desc=\"Total Progress\", position=0, leave=True) as pbar:\n",
    "            last_count = 0\n",
    "            while any(p.is_alive() for p in tasks):  # Update while processes are running\n",
    "                current_count = progress_bar.value\n",
    "                pbar.update(current_count - last_count)  # Update tqdm only for new progress\n",
    "                last_count = current_count\n",
    "                time.sleep(1)  # Prevents excessive updates\n",
    "\n",
    "        # await asyncio.gather(*tasks)\n",
    "        for p in tasks:\n",
    "            p.join()  # Wait for all processes to finish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ffddc6-e199-4183-a069-d71bb78d4a11",
   "metadata": {},
   "source": [
    "### Generate the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73045b6e-eb02-4ba4-98a5-32cd1a6ec564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "loaded processor.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511e2cee4f864500950285897badf172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from cvla.hf_model_class import cVLA_wrapped\n",
    "from pathlib import Path\n",
    "#v17 = \"cvla-clevr-camRF-sceneR-9__img_1_pr_interleaved_enc_xyzrotvec-cam-512xy128dmaxTokens13_lr3e-05_sortAll_augs_max20k_2025-04-27_18-00-52\" # 6 - 20\n",
    "#model_path = Path(\"/work/dlclarge2/bratulic-cvla/models/\") / v17 / \"checkpoint-19000\"\n",
    "model_location = Path(\"/data/lmbraid19/argusm/models/_text_lr3e-05xyzrotvec-cam-512xy256d_2025-04-23_12-03-48\")/\n",
    "model_path = Path(\"/work/dlclarge2/bratulic-cvla/models\")  / \"_text_lr3e-05_enc512_128d_depth_2025-04-29_10-38-15\" / \"checkpoint-4687\"\n",
    "model = cVLA_wrapped(model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a2b89c8e-7fd5-45c5-85cf-729d281c978b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                               | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action encoder xyzrotvec-cam-512xy128d\n",
      "filter visible objects True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                               | 0/5 [00:03<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([128, 128, 3])\n",
      "<class 'tuple'>\n",
      "torch.Size([448, 448, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(args.N_samples)):\n\u001b[32m     27\u001b[39m             \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m                 _ = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43menv_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m     30\u001b[39m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 362\u001b[39m, in \u001b[36miterate_env\u001b[39m\u001b[34m(args, vis, model)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;28mprint\u001b[39m(image_before[\u001b[32m1\u001b[39m].shape)\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model:\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m     _, _, _, token_pred = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_top\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    363\u001b[39m     json_dict[\u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m] = token_pred\n\u001b[32m    364\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m token_pred == \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m token_pred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/dlclarge2/zhangj-zhangj-CFM/cVLA/cvla/hf_model_class.py:266\u001b[39m, in \u001b[36mcVLA_wrapped.make_predictions\u001b[39m\u001b[34m(self, image_before, prefix)\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmake_predictions\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_before, prefix):\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     decoded = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_before\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, decoded[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/dlclarge2/zhangj-zhangj-CFM/cVLA/cvla/hf_model_class.py:234\u001b[39m, in \u001b[36mcVLA_wrapped.predict\u001b[39m\u001b[34m(self, images, prefix, robot_state)\u001b[39m\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    232\u001b[39m     entry_dict = \u001b[38;5;28mdict\u001b[39m(prefix=prefix)\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m     depth, image = images\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_depth:\n\u001b[32m    236\u001b[39m         depth = depth.detach().cpu().numpy().squeeze()\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# For batch data collection without visualization\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "args = Args(\n",
    "    robot_uids=\"panda_wristcam\",\n",
    "    record_dir=\"/work/dlclarge2/zhangj-zhangj-CFM/data\",\n",
    "    N_samples=5,                # Number of samples to generate\n",
    "    object_dataset=\"objaverse\",\n",
    "    shader=\"default\",\n",
    "    obs_mode=\"rgb+depth+segmentation\",\n",
    "    render_mode=\"rgb_array\",              # Disable visualization for speed\n",
    "    num_envs=1,\n",
    "    run_mode = \"script\",\n",
    "    quiet = True,\n",
    "    action_encoder= model.enc_model.NAME\n",
    ")\n",
    "\n",
    "inital_seed = 2919129908\n",
    "random.seed(inital_seed)\n",
    "seeds = random.sample(range(0, 2**32), args.N_samples)\n",
    "#args.seed = seeds\n",
    "\n",
    "env_iter = iterate_env(args, vis=False, model=model)\n",
    "for _ in tqdm(range(args.N_samples)):\n",
    "            try:\n",
    "                _ = next(env_iter)\n",
    "            except StopIteration:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fd0232-9eac-4017-b320-bed7d792ef20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7eb32718-c7df-4010-bb22-110749da21d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('20250522_155344.h5', 113795094), ('20250522_155434.h5', 70876296), ('20250522_160249.h5', 36802758), ('20250522_164107.h5', 157719848), ('20250522_164759.h5', 41284642), ('20250522_165411.h5', 800), ('20250522_170208.h5', 800), ('20250522_170301.h5', 800), ('20250522_170528.h5', 800), ('20250522_170734.h5', 157719848), ('20250522_171136.h5', 159584224), ('20250522_172903.h5', 800), ('20250522_173526.h5', 159584224), ('20250522_173628.h5', 159584224), ('20250522_173937.h5', 800), ('20250522_174037.h5', 159584224), ('20250522_174155.h5', 159584224), ('20250522_174501.h5', 800), ('20250522_174544.h5', 55001646), ('20250522_174648.h5', 800), ('20250522_174759.h5', 96), ('20250522_175704.h5', 159584224)]\n",
      "Largest .h5 file: 20250522_171136.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "video_dir = \"/work/dlclarge2/zhangj-zhangj-CFM/data\"\n",
    "\n",
    "# Get list of .h5 files with their sizes\n",
    "h5_files = [\n",
    "    (f, os.path.getsize(os.path.join(video_dir, f)))\n",
    "    for f in os.listdir(video_dir)\n",
    "    if f.endswith('.h5')\n",
    "]\n",
    "print(h5_files)\n",
    "\n",
    "# Select the file with the maximum size\n",
    "if h5_files:\n",
    "    largest_file = max(h5_files, key=lambda x: x[1])[0]\n",
    "    print(f\"Largest .h5 file: {largest_file}\")\n",
    "else:\n",
    "    print(\"No .h5 files found.\")\n",
    "#Largest .h5 file: 20250522_155344.h5//20250522_164107.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4162cc93-418f-4462-b1dc-e58be617bd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9acd3f7a31417e8aedcac997a45c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to /work/dlclarge2/zhangj-zhangj-CFM/data/videos/video_test5222_traj_0.mp4\n",
      "Video saved to /work/dlclarge2/zhangj-zhangj-CFM/data/videos/video_test5222_traj_1.mp4\n",
      "Video saved to /work/dlclarge2/zhangj-zhangj-CFM/data/videos/video_test5222_traj_2.mp4\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from cvla.data_loader_h5 import H5Dataset\n",
    "\n",
    "dataset_location = Path(video_dir) / \"20250522_175704.h5\"\n",
    "dataset = H5Dataset(dataset_location)\n",
    "video_dir = Path(video_dir)/ \"videos\"\n",
    "output_path = video_dir / \"run_0.mp4\"\n",
    "\n",
    "for key in tqdm(sorted(dataset.h5_file.keys())):\n",
    "    frames = dataset.h5_file[f\"{key}/obs/sensor_data/render_camera/rgb\"]\n",
    "\n",
    "    # Define video parameters\n",
    "    height, width = frames.shape[1:3]\n",
    "    fps = 30  # or whatever frame rate you want\n",
    "    output_path = video_dir / f'video_test5222_{key}.mp4'\n",
    "\n",
    "    # Define the video writer using MP4 codec\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # or 'avc1'\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Write each frame\n",
    "    for frame in frames:\n",
    "        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # OpenCV expects BGR\n",
    "        out.write(frame_bgr)\n",
    "\n",
    "    out.release()\n",
    "    print(f\"Video saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf1f8c37-6ade-4e7e-8798-cdd97dc71261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('/work/dlclarge2/zhangj-zhangj-CFM/data/0.mp4'), PosixPath('/work/dlclarge2/zhangj-zhangj-CFM/data/1.mp4'), PosixPath('/work/dlclarge2/zhangj-zhangj-CFM/data/2.mp4'), PosixPath('/work/dlclarge2/zhangj-zhangj-CFM/data/3.mp4'), PosixPath('/work/dlclarge2/zhangj-zhangj-CFM/data/4.mp4'), PosixPath('/work/dlclarge2/zhangj-zhangj-CFM/data/5.mp4'), PosixPath('/work/dlclarge2/zhangj-zhangj-CFM/data/6.mp4'), PosixPath('/work/dlclarge2/zhangj-zhangj-CFM/data/7.mp4'), PosixPath('/work/dlclarge2/zhangj-zhangj-CFM/data/8.mp4'), PosixPath('/work/dlclarge2/zhangj-zhangj-CFM/data/9.mp4'), PosixPath('/work/dlclarge2/zhangj-zhangj-CFM/data/10.mp4'), PosixPath('/work/dlclarge2/zhangj-zhangj-CFM/data/11.mp4'), PosixPath('/work/dlclarge2/zhangj-zhangj-CFM/data/12.mp4'), PosixPath('/work/dlclarge2/zhangj-zhangj-CFM/data/13.mp4'), PosixPath('/work/dlclarge2/zhangj-zhangj-CFM/data/videos/video_test520_traj_0.mp4'), PosixPath('/work/dlclarge2/zhangj-zhangj-CFM/data/videos/video_test520_traj_1.mp4'), PosixPath('/work/dlclarge2/zhangj-zhangj-CFM/data/videos/video_test520_traj_2.mp4'), PosixPath('/work/dlclarge2/zhangj-zhangj-CFM/data/videos/video_test522_traj_0.mp4'), PosixPath('/work/dlclarge2/zhangj-zhangj-CFM/data/videos/video_test522_traj_1.mp4'), PosixPath('/work/dlclarge2/zhangj-zhangj-CFM/data/videos/video_test522_traj_2.mp4'), PosixPath('/work/dlclarge2/zhangj-zhangj-CFM/data/videos/video_test522_traj_3.mp4')]\n"
     ]
    }
   ],
   "source": [
    "print(list(Path(args.record_dir).rglob(\"*.mp4\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84fa913-d805-4ce9-a717-0c93306fefb4",
   "metadata": {},
   "source": [
    "### In time visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57ee2b8d-82f3-4542-935f-375eb53820e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Create window failed: Renderer does not support display.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m5\u001b[39m):  \u001b[38;5;66;03m# Run 5 episodes\u001b[39;00m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m         image_before, json_dict, seed = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43menv_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m         \u001b[38;5;66;03m# You can process the outputs here\u001b[39;00m\n\u001b[32m     28\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCompleted episode with seed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 168\u001b[39m, in \u001b[36miterate_env\u001b[39m\u001b[34m(args, vis, model)\u001b[39m\n\u001b[32m    162\u001b[39m         env = RecordEpisode(env, args.record_dir, info_on_video=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    163\u001b[39m                             save_trajectory=\u001b[38;5;28;01mTrue\u001b[39;00m, max_steps_per_video=env._max_episode_steps,\n\u001b[32m    164\u001b[39m                             save_on_reset=SAVE_FREQ == \u001b[32m1\u001b[39m,\n\u001b[32m    165\u001b[39m                             record_env_state=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m env = \u001b[43mmake_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m    171\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mObservation space\u001b[39m\u001b[33m\"\u001b[39m, env.observation_space)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 141\u001b[39m, in \u001b[36miterate_env.<locals>.make_env\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmake_env\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     env = \u001b[43mgym\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43menv_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobs_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobs_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreward_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreward_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontrol_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontrol_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m        \u001b[49m\u001b[43msensor_configs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshader_pack\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhuman_render_camera_configs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshader_pack\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mviewer_camera_configs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshader_pack\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m        \u001b[49m\u001b[43msim_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43msim_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparallel_in_single_scene\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparallel_in_single_scene\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrobot_uids\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrobot_uids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscene_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscene_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobject_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobject_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcamera_views\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcamera_views\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscene_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscene_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# camera_cfgs={\"use_stereo_depth\": True, },\u001b[39;49;00m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# **args.env_kwargs\u001b[39;49;00m\n\u001b[32m    160\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m args.record_dir:\n\u001b[32m    162\u001b[39m         env = RecordEpisode(env, args.record_dir, info_on_video=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    163\u001b[39m                             save_trajectory=\u001b[38;5;28;01mTrue\u001b[39;00m, max_steps_per_video=env._max_episode_steps,\n\u001b[32m    164\u001b[39m                             save_on_reset=SAVE_FREQ == \u001b[32m1\u001b[39m,\n\u001b[32m    165\u001b[39m                             record_env_state=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mani_skill/lib/python3.12/site-packages/gymnasium/envs/registration.py:802\u001b[39m, in \u001b[36mmake\u001b[39m\u001b[34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[39m\n\u001b[32m    799\u001b[39m     render_mode = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m802\u001b[39m     env = \u001b[43menv_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43menv_spec_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    804\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    805\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e).find(\u001b[33m\"\u001b[39m\u001b[33mgot an unexpected keyword argument \u001b[39m\u001b[33m'\u001b[39m\u001b[33mrender_mode\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m) >= \u001b[32m0\u001b[39m\n\u001b[32m    806\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m apply_human_rendering\n\u001b[32m    807\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/dlclarge2/zhangj-zhangj-CFM/ManiSkill/mani_skill/utils/registration.py:182\u001b[39m, in \u001b[36mmake\u001b[39m\u001b[34m(env_id, **kwargs)\u001b[39m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mEnv \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m not found in registry\u001b[39m\u001b[33m\"\u001b[39m.format(env_id))\n\u001b[32m    181\u001b[39m env_spec = REGISTERED_ENVS[env_id]\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m env = \u001b[43menv_spec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m env\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/dlclarge2/zhangj-zhangj-CFM/ManiSkill/mani_skill/utils/registration.py:79\u001b[39m, in \u001b[36mEnvSpec.make\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExiting as assets are not found or downloaded\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     78\u001b[39m         exit()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcls\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/dlclarge2/zhangj-zhangj-CFM/ManiSkill/mani_skill/examples/cvla/cvla_env.py:64\u001b[39m, in \u001b[36mCvlaMoveEnv.__init__\u001b[39m\u001b[34m(self, robot_uids, scene_dataset, object_dataset, camera_views, scene_options, robot_init_qpos_noise, *args, **kwargs)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28mself\u001b[39m.spoc_dataset = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mself\u001b[39m.initalize_render_camera()  \u001b[38;5;66;03m# sets render_camera_config\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrobot_uids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrobot_uids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/dlclarge2/zhangj-zhangj-CFM/ManiSkill/mani_skill/envs/sapien_env.py:312\u001b[39m, in \u001b[36mBaseEnv.__init__\u001b[39m\u001b[34m(self, num_envs, obs_mode, reward_mode, control_mode, render_mode, shader_dir, enable_shadow, sensor_configs, human_render_camera_configs, viewer_camera_configs, robot_uids, sim_config, reconfiguration_freq, sim_backend, render_backend, parallel_in_single_scene, enhanced_determinism)\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;28mself\u001b[39m._set_main_rng([\u001b[32m2022\u001b[39m + i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_envs)])\n\u001b[32m    309\u001b[39m \u001b[38;5;28mself\u001b[39m._elapsed_steps = (\n\u001b[32m    310\u001b[39m     torch.zeros(\u001b[38;5;28mself\u001b[39m.num_envs, device=\u001b[38;5;28mself\u001b[39m.device, dtype=torch.int32)\n\u001b[32m    311\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m obs, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2022\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreconfigure\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[38;5;28mself\u001b[39m._init_raw_obs = common.to_cpu_tensor(obs)\n\u001b[32m    315\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"the raw observation returned by the env.reset (a cpu torch tensor/dict of tensors). Useful for future observation wrappers to use to auto generate observation spaces\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/dlclarge2/zhangj-zhangj-CFM/ManiSkill/mani_skill/examples/cvla/cvla_env.py:453\u001b[39m, in \u001b[36mCvlaMoveEnv.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    451\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    452\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/dlclarge2/zhangj-zhangj-CFM/ManiSkill/mani_skill/envs/sapien_env.py:890\u001b[39m, in \u001b[36mBaseEnv.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    888\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.random.fork_rng():\n\u001b[32m    889\u001b[39m     torch.manual_seed(seed=\u001b[38;5;28mself\u001b[39m._episode_seed[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m890\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reconfigure\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    891\u001b[39m     \u001b[38;5;28mself\u001b[39m._after_reconfigure(options)\n\u001b[32m    892\u001b[39m \u001b[38;5;66;03m# Set the episode rng again after reconfiguration to guarantee seed reproducibility\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/dlclarge2/zhangj-zhangj-CFM/ManiSkill/mani_skill/envs/sapien_env.py:735\u001b[39m, in \u001b[36mBaseEnv._reconfigure\u001b[39m\u001b[34m(self, options)\u001b[39m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._setup_sensors(options)\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode == \u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._viewer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m735\u001b[39m     \u001b[38;5;28mself\u001b[39m._viewer = \u001b[43mcreate_viewer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_viewer_camera_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._viewer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._setup_viewer()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/dlclarge2/zhangj-zhangj-CFM/ManiSkill/mani_skill/viewer/__init__.py:31\u001b[39m, in \u001b[36mcreate_viewer\u001b[39m\u001b[34m(viewer_camera_config)\u001b[39m\n\u001b[32m     21\u001b[39m     sapien.render.set_ray_tracing_path_depth(\n\u001b[32m     22\u001b[39m         viewer_camera_config.shader_config.shader_pack_config[\n\u001b[32m     23\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mray_tracing_path_depth\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     24\u001b[39m         ]\n\u001b[32m     25\u001b[39m     )\n\u001b[32m     26\u001b[39m     sapien.render.set_ray_tracing_samples_per_pixel(\n\u001b[32m     27\u001b[39m         viewer_camera_config.shader_config.shader_pack_config[\n\u001b[32m     28\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mray_tracing_samples_per_pixel\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m         ]\n\u001b[32m     30\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m viewer = \u001b[43mViewer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolutions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mviewer_camera_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mviewer_camera_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys.platform == \u001b[33m'\u001b[39m\u001b[33mdarwin\u001b[39m\u001b[33m'\u001b[39m:  \u001b[38;5;66;03m# macOS\u001b[39;00m\n\u001b[32m     35\u001b[39m     viewer.window.set_content_scale(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mani_skill/lib/python3.12/site-packages/sapien/utils/viewer/viewer.py:73\u001b[39m, in \u001b[36mViewer.__init__\u001b[39m\u001b[34m(self, renderer, shader_dir, resolutions, plugins)\u001b[39m\n\u001b[32m     69\u001b[39m resolution = np.array(resolutions).flatten()[:\u001b[32m2\u001b[39m]\n\u001b[32m     71\u001b[39m \u001b[38;5;28mself\u001b[39m.scenes = []\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28mself\u001b[39m.window = \u001b[43mRenderWindow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshader_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28mself\u001b[39m.window.set_focus_callback(\u001b[38;5;28mself\u001b[39m.focus_change)\n\u001b[32m     75\u001b[39m \u001b[38;5;28mself\u001b[39m.window.set_drop_callback(\u001b[38;5;28mself\u001b[39m.drop)\n",
      "\u001b[31mRuntimeError\u001b[39m: Create window failed: Renderer does not support display."
     ]
    }
   ],
   "source": [
    "from dataclasses import asdict\n",
    "import json\n",
    "\n",
    "# 1. Create an Args object with your desired parameters\n",
    "args = Args(\n",
    "    env_id=\"CvlaMove-v1\",\n",
    "    record_dir=\"/work/dlclarge2/zhangj-zhangj-CFM/data\",  # Required for saving\n",
    "    N_samples=50,                    # Number of samples to generate\n",
    "    object_dataset=\"objaverse\",          # or \"ycb\" or \"objaverse\"\n",
    "    shader=\"default\",                # or \"rt\" for ray tracing\n",
    "    obs_mode=\"rgb+depth+segmentation\",\n",
    "    control_mode=\"pd_joint_pos\",\n",
    "    # Add other parameters as needed\n",
    ")\n",
    "\n",
    "# 2. For visualization (optional)\n",
    "args.render_mode = \"human\"  # Enable visualization\n",
    "args.num_envs = 1           # For visualization, keep at 1\n",
    "\n",
    "# 3. Run the environment iteration\n",
    "env_iter = iterate_env(args, vis=True)\n",
    "\n",
    "# 4. To run a specific number of episodes:\n",
    "for _ in range(5):  # Run 5 episodes\n",
    "    try:\n",
    "        image_before, json_dict, seed = next(env_iter)\n",
    "        # You can process the outputs here\n",
    "        print(f\"Completed episode with seed {seed}\")\n",
    "    except StopIteration:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a4515a-e194-4ccb-ad57-77fc04a2cbd8",
   "metadata": {},
   "source": [
    "### origin code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790365b4-19aa-458c-a9ad-dc1d43dfba49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
