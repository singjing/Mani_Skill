{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05af6d42-c4a4-4f05-ae7d-7f1d10239b8f",
   "metadata": {},
   "source": [
    "# Update by the top view imaage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16acf8d6-255d-4180-89b7-f9371ce730d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_view = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "716f20ca-0ad8-45cc-8c5c-d27b30eabc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main entry point for running the CVLA environment. See readme for details.\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import traceback\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Annotated, Union\n",
    "\n",
    "import tyro\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import gymnasium as gym\n",
    "import sapien\n",
    "\n",
    "from mani_skill.utils.structs import Pose\n",
    "from mani_skill.utils.wrappers import RecordEpisode\n",
    "import mani_skill.examples.cvla.cvla_env  # do import to register env, not used otherwise\n",
    "from mani_skill.examples.cvla.utils_trajectory import generate_curve_torch, DummyCamera\n",
    "from mani_skill.examples.cvla.utils_traj_tokens import getActionEncInstance, to_prefix_suffix\n",
    "from mani_skill.examples.cvla.utils_record import apply_check_object_pixels_obs\n",
    "from mani_skill.examples.cvla.utils_record import downcast_seg_array\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "\n",
    "RAND_MAX = 2**32 - 1\n",
    "SAVE_FREQ = 1  # save after every reset\n",
    "RESET_HARD = 10  # re-start environment after every n steps\n",
    "SAVE_VIDEO = False  # save videos\n",
    "# minimum percentage of image that must be object, set to None to disable checking\n",
    "MIN_OBJ_VISIBLE_PERCENT = 0.5\n",
    "\n",
    "\n",
    "def getMotionPlanner(env):\n",
    "    if env.unwrapped.robot_uids in (\"panda\", \"panda_wristcam\"):\n",
    "        from mani_skill.examples.motionplanning.panda.motionplanner import \\\n",
    "            PandaArmMotionPlanningSolver as RobotArmMotionPlanningSolver\n",
    "    elif env.unwrapped.robot_uids == \"fetch\":\n",
    "        from mani_skill.examples.motionplanning.fetch.motionplanner import \\\n",
    "            FetchArmMotionPlanningSolver as RobotArmMotionPlanningSolver\n",
    "    else:\n",
    "        raise ValueError(f\"no motion planner for {env.unwrapped.robot_uids}\")\n",
    "    return RobotArmMotionPlanningSolver\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    env_id: Annotated[str, tyro.conf.arg(aliases=[\"-e\"])] = \"CvlaMove-v1\"\n",
    "    \"\"\"The environment ID of the task you want to simulate\"\"\"\n",
    "\n",
    "    obs_mode: Annotated[str, tyro.conf.arg(aliases=[\"-o\"])] = \"rgb+depth+segmentation\"\n",
    "    \"\"\"Observation mode\"\"\"\n",
    "\n",
    "    sim_backend: Annotated[str, tyro.conf.arg(aliases=[\"-b\"])] = \"auto\"\n",
    "    \"\"\"Which simulation backend to use. Can be 'auto', 'cpu', 'gpu'\"\"\"\n",
    "\n",
    "    reward_mode: Optional[str] = None\n",
    "    \"\"\"Reward mode\"\"\"\n",
    "\n",
    "    num_envs: int = 1\n",
    "    \"\"\"Number of environments to run.\"\"\"\n",
    "\n",
    "    control_mode: Annotated[Optional[str], tyro.conf.arg(aliases=[\"-c\"])] = \"pd_joint_pos\"\n",
    "    \"\"\"Control mode\"\"\"\n",
    "\n",
    "    render_mode: str = \"rgb_array\"\n",
    "    \"\"\"Render mode\"\"\"\n",
    "\n",
    "    shader: str = \"default\"\n",
    "    \"\"\"Change shader used for all cameras in the environment for rendering. Default is 'minimal' which is very fast. Can also be 'rt' for ray tracing and generating photo-realistic renders. Can also be 'rt-fast' for a faster but lower quality ray-traced renderer\"\"\"\n",
    "\n",
    "    record_dir: Optional[str] = None\n",
    "    \"\"\"Directory to save recordings\"\"\"\n",
    "\n",
    "    pause: Annotated[bool, tyro.conf.arg(aliases=[\"-p\"])] = False\n",
    "    \"\"\"If using human render mode, auto pauses the simulation upon loading\"\"\"\n",
    "\n",
    "    quiet: bool = False\n",
    "    \"\"\"Disable verbose output.\"\"\"\n",
    "\n",
    "    seed: Annotated[Optional[Union[int, List[int], str]], tyro.conf.arg(aliases=[\"-s\"])] = None\n",
    "    \"\"\"Seed(s) for random actions and simulator. Can be a single integer or a list of integers. Default is None (no seeds)\"\"\"\n",
    "\n",
    "    run_mode: Annotated[Optional[str], tyro.conf.arg(aliases=[\"-m\"])] = \"script\"\n",
    "    \"\"\"Run mode, options are script, interactive, first\"\"\"\n",
    "\n",
    "    robot_uids: Annotated[Optional[str], tyro.conf.arg(aliases=[\"-r\"])] = \"panda\"\n",
    "    \"\"\"Robots, options are: panda, panda_wristcam, xarm6_robotiq, floating_inspire_hand_right\"\"\"\n",
    "\n",
    "    scene_dataset: Annotated[Optional[str], tyro.conf.arg(aliases=[\"-sd\"])] = \"Table\"\n",
    "    \"\"\"Scene datasets: options are: Table, ProcTHOR\"\"\"\n",
    "\n",
    "    scene_options: Annotated[Optional[str], tyro.conf.arg(aliases=[\"-so\"])] = \"fixed\"\n",
    "    \"\"\"Randomize the scene\"\"\"\n",
    "\n",
    "    object_dataset: Annotated[Optional[str], tyro.conf.arg(aliases=[\"-od\"])] = \"clevr\"\n",
    "    \"\"\"Dataset from which we sample objects, options are: clevr, ycb, objaverse\"\"\"\n",
    "\n",
    "    camera_views: Annotated[Optional[str], tyro.conf.arg(aliases=[\"-cv\"])] = \"random_side\"\n",
    "    \"\"\"Dataset from which we sample objects\"\"\"\n",
    "\n",
    "    action_encoder: Annotated[Optional[str], tyro.conf.arg(aliases=[\"-ae\"])] = \"xyzrotvec-cam-1024xy\"\n",
    "    \"\"\"Action encoding\"\"\"\n",
    "\n",
    "    N_samples: Annotated[Optional[int], tyro.conf.arg(aliases=[\"-N\"])] = 50\n",
    "    \"\"\"Number of samples\"\"\"\n",
    "\n",
    "\n",
    "def reset_random(args, orig_seeds):\n",
    "    if orig_seeds is None:\n",
    "        seed = random.randrange(RAND_MAX)\n",
    "    elif isinstance(orig_seeds, list):\n",
    "        seed = orig_seeds.pop()\n",
    "    elif isinstance(orig_seeds, int):\n",
    "        seed = orig_seeds\n",
    "    else:\n",
    "        raise ValueError\n",
    "    args.seed = [seed]\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def iterate_env(args: Args, vis=True, model=None):\n",
    "    np.set_printoptions(suppress=True, precision=3)\n",
    "    verbose = not args.quiet\n",
    "    parallel_in_single_scene = args.render_mode == \"human\"\n",
    "    if args.render_mode == \"human\" and args.obs_mode in [\"sensor_data\", \"rgb\", \"rgbd\", \"depth\", \"point_cloud\", \"top_view\"]:\n",
    "        print(\"Disabling parallel single scene/GUI render as observation mode is a visual one. Change observation mode to state or state_dict to see a parallel env render\")\n",
    "        parallel_in_single_scene = False\n",
    "    if args.render_mode == \"human\" and args.num_envs == 1:\n",
    "        parallel_in_single_scene = False\n",
    "\n",
    "    # define make env as a function to enable hard resets\n",
    "    def make_env():\n",
    "        env = gym.make(\n",
    "            args.env_id,\n",
    "            obs_mode=args.obs_mode,\n",
    "            reward_mode=args.reward_mode,\n",
    "            control_mode=args.control_mode,\n",
    "            render_mode=args.render_mode,\n",
    "            sensor_configs=dict(shader_pack=args.shader),\n",
    "            human_render_camera_configs=dict(shader_pack=args.shader),\n",
    "            viewer_camera_configs=dict(shader_pack=args.shader),\n",
    "            num_envs=args.num_envs,\n",
    "            sim_backend=args.sim_backend,\n",
    "            parallel_in_single_scene=parallel_in_single_scene,\n",
    "            robot_uids=args.robot_uids,\n",
    "            scene_dataset=args.scene_dataset,\n",
    "            object_dataset=args.object_dataset,\n",
    "            camera_views=args.camera_views,\n",
    "            scene_options=args.scene_options,\n",
    "            # camera_cfgs={\"use_stereo_depth\": True, },\n",
    "            # **args.env_kwargs\n",
    "        )\n",
    "        if args.record_dir:\n",
    "            env = RecordEpisode(env, args.record_dir, info_on_video=False,\n",
    "                                save_trajectory=True, max_steps_per_video=env._max_episode_steps,\n",
    "                                save_on_reset=SAVE_FREQ == 1,\n",
    "                                record_env_state=True)\n",
    "        return env\n",
    "\n",
    "    env = make_env()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Observation space\", env.observation_space)\n",
    "        print(\"Action space\", env.action_space)\n",
    "        print(\"Control mode\", env.unwrapped.control_mode)\n",
    "        print(\"Reward mode\", env.unwrapped.reward_mode)\n",
    "        print(\"Render mode\", args.render_mode)\n",
    "        print(\"Obs mode\", args.obs_mode)\n",
    "\n",
    "    filter_visible = True\n",
    "    action_encoder = getActionEncInstance(args.action_encoder)\n",
    "    enc_func, dec_func = action_encoder.encode_trajectory, action_encoder.decode_trajectory\n",
    "\n",
    "    print(\"action encoder\", args.action_encoder)\n",
    "    print(\"filter visible objects\", filter_visible)\n",
    "\n",
    "    orig_seeds = args.seed\n",
    "    N_valid_samples = 0\n",
    "    max_attempts = 10**6\n",
    "    for i in range(max_attempts):\n",
    "        reset_random(args, orig_seeds)\n",
    "        assert isinstance(args.seed, list)\n",
    "\n",
    "        if i != 0 and i % RESET_HARD == 0:\n",
    "            del env\n",
    "            env = make_env()\n",
    "        try:\n",
    "            obs, _ = env.reset(seed=args.seed[0], options=dict(reconfigure=True))\n",
    "        except Exception as e:  # Catch all exceptions, including AssertionError\n",
    "            print(f\"Encountered error {e.__class__.__name__} at seed {args.seed[0]} while resetting env. Skipping this iteration.\")\n",
    "            print(e)\n",
    "            traceback.print_exc()  # Prints the full traceback\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "\n",
    "        if MIN_OBJ_VISIBLE_PERCENT is None:\n",
    "            obj_are_vis = True\n",
    "        elif \"top\" in str(args.camera_views): # from the top-view above the fisrt object, can't see two objects\n",
    "            obj_are_vis = True\n",
    "        else:\n",
    "            obj_are_vis = apply_check_object_pixels_obs(obs, env, N_percent=MIN_OBJ_VISIBLE_PERCENT)\n",
    "        if not obj_are_vis:\n",
    "            print(\"Warning: object not visible, skipping sample\")\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "\n",
    "        # Note: when using RecordEpisode this will create 20x the number of saved frames\n",
    "        # so 75GB -> 1.5 TB, which is no good.\n",
    "        # Let the objects settle (!)\n",
    "        # for _ in range(20):\n",
    "        #    _ = env.step(obs[\"agent\"][\"qpos\"][..., :8])\n",
    "        \n",
    "            \n",
    "        if args.seed is not None:\n",
    "            env.action_space.seed(args.seed[0])\n",
    "        if vis and args.render_mode is not None:\n",
    "            viewer = env.render()\n",
    "            if isinstance(viewer, sapien.utils.Viewer):\n",
    "                viewer.paused = args.pause\n",
    "            env.render()\n",
    "        else:\n",
    "            env.render()\n",
    "\n",
    "        # Not parrelized\n",
    "        # env_idx = 0\n",
    "\n",
    "        # -----\n",
    "        # Warning, taking an image form obs/rendering it results in different calibrations!\n",
    "        # e.g. images = env.base_env.scene.get_human_render_camera_images('render_camera')\n",
    "        # -----\n",
    "        obj_start = Pose(obs[\"extra\"][\"obj_start\"].clone().detach())\n",
    "        obj_end = Pose(obs[\"extra\"][\"obj_end\"].clone().detach())\n",
    "        grasp_pose = Pose(obs[\"extra\"][\"grasp_pose\"].clone().detach())\n",
    "        tcp_pose = Pose(obs[\"extra\"][\"tcp_pose\"].clone().detach())\n",
    "        robot_pose = Pose(obs[\"extra\"][\"robot_pose\"].clone().detach())\n",
    "\n",
    "        #print(\"\n",
    "        print(\"obj_st\")\n",
    "        print(obj_start)\n",
    "        print(\"grasp_pose\")\n",
    "        print(grasp_pose)\n",
    "\n",
    "        try:\n",
    "            camera_intrinsic = obs[\"sensor_param\"][\"render_camera\"][\"intrinsic_cv\"].clone().detach()\n",
    "            camera_extrinsic = obs[\"sensor_param\"][\"render_camera\"][\"extrinsic_cv\"].clone().detach()\n",
    "            image_before = obs[\"sensor_data\"][\"render_camera\"][\"rgb\"][0].clone().detach()\n",
    "            depth = obs[\"sensor_data\"][\"render_camera\"][\"depth\"][0].clone().detach()\n",
    "            width, height, _ = image_before.shape\n",
    "            camera = DummyCamera(camera_intrinsic, camera_extrinsic, width, height)\n",
    "            # add depth to image_before if this mode take depth\n",
    "            if \"depth\" in str(args.obs_mode) and \"top\" not in str(args.camera_views):\n",
    "                image_before = (depth, image_before)\n",
    "        except KeyError:\n",
    "            image_before = obs[\"sensor_data\"][\"render_camera\"][\"rgb\"][0].clone().detach()\n",
    "            camera = env.base_env.scene.human_render_cameras['render_camera'].camera\n",
    "\n",
    "        action_text = env.unwrapped.get_obs_scene()[\"text\"]\n",
    "        assert isinstance(action_text, str) and action_text not in (None, \"\"), f\"action_text: {action_text}\"\n",
    "\n",
    "        prefix, token_str, curve_3d, orns_3d, info = to_prefix_suffix(obj_start, obj_end,\n",
    "                                                                      camera, grasp_pose, tcp_pose,\n",
    "                                                                      action_text, enc_func, robot_pose=robot_pose)\n",
    "        top_view.append(image_before)\n",
    "        json_dict = dict(prefix=prefix, suffix=token_str,\n",
    "                         action_text=action_text,\n",
    "                         camera_extrinsic=camera.get_extrinsic_matrix().detach().numpy().tolist(),\n",
    "                         camera_intrinsic=camera.get_intrinsic_matrix().detach().numpy().tolist(),\n",
    "                         obj_start_pose=obj_start.raw_pose.detach().numpy().tolist(),\n",
    "                         obj_end_pose=obj_end.raw_pose.detach().numpy().tolist(),\n",
    "                         robot_pose=robot_pose.raw_pose.detach().numpy().tolist(),\n",
    "                         tcp_start_pose=tcp_pose.raw_pose.detach().numpy().tolist(),\n",
    "                         grasp_pose=grasp_pose.raw_pose.detach().numpy().tolist(),\n",
    "                         info=info,\n",
    "                         seed=args.seed[0],\n",
    "                         iter_reached=i,\n",
    "                         )\n",
    "\n",
    "        encode_decode_trajectory = True\n",
    "        if encode_decode_trajectory:\n",
    "            curve_3d_est, orns_3d_est = dec_func(token_str, camera, robot_pose=robot_pose)\n",
    "            curve_3d = curve_3d_est  # set the unparsed trajectory one used for policy\n",
    "            orns_3d = orns_3d_est\n",
    "        def get_pose_of_new_predicts(image,prefix,model):\n",
    "            pose_list = []\n",
    "            return pose_list\n",
    "\n",
    "        # Evaluate the trajectory\n",
    "        if args.run_mode == \"script\" or model:\n",
    "            assert args.control_mode == \"pd_joint_pos\"\n",
    "            if verbose and info[\"didclip_traj\"]:\n",
    "                print(\"Warning refered object out of camera view.\")\n",
    "\n",
    "            if model:\n",
    "                '''\n",
    "                print(f\"prefix:{prefix}\")\n",
    "                print(f\"image type:{type(image_before)}\")\n",
    "                print(\"visualize of image before\")\n",
    "                show_before = image_before.cpu().numpy()\n",
    "                plt.imshow(show_before)\n",
    "                plt.axis('off')  # Turn off axis numbers/labels\n",
    "                plt.show()\n",
    "                '''\n",
    "                _, _, _, token_pred = model.make_predictions(image_before, prefix)\n",
    "                json_dict[\"prediction\"] = token_pred\n",
    "                if token_pred == \"\" or token_pred is None:\n",
    "                    print(\"Warning: empty prediction, failing\")\n",
    "                    json_dict[\"reward\"] = 0\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    yield image_before, json_dict, args.seed[0]\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    curve_3d_pred, orns_3d_pred = dec_func(token_pred, camera=camera, robot_pose=robot_pose)\n",
    "                    curve_3d = curve_3d_pred  # set the unparsed trajectory one used for policy\n",
    "                    orns_3d = orns_3d_pred\n",
    "                # TODO(max): this should only catch value errors\n",
    "                except:\n",
    "                    print(\"Warning: exception during decoding tokens, failing\", token_pred)\n",
    "                    json_dict[\"reward\"] = 0\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    yield image_before, json_dict, args.seed[0]\n",
    "                    continue\n",
    "\n",
    "            # start and stop poses\n",
    "            if curve_3d.shape[1] != 2 or orns_3d.shape[1] != 2:\n",
    "                print(\"Warning: Model decoded something that is not a valid trajectory\")\n",
    "                json_dict[\"reward\"] = 0.0\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                yield image_before, json_dict, args.seed[0]\n",
    "                N_valid_samples += 1\n",
    "                continue\n",
    "            \n",
    "            # convert two keypoints into motion sequence\n",
    "            _, curve_3d_i = generate_curve_torch(curve_3d[:, 0], curve_3d[:, -1], num_points=3)\n",
    "            grasp_pose = Pose.create_from_pq(p=curve_3d[:, 0], q=orns_3d[:, 0])\n",
    "            reach_pose = grasp_pose * sapien.Pose([0, 0, -0.10])  # Go above the object before grasping\n",
    "            lift_pose = Pose.create_from_pq(p=curve_3d_i[:, 1], q=orns_3d[:, 1])\n",
    "            align_pose = Pose.create_from_pq(p=curve_3d_i[:, 2], q=orns_3d[:, 1])\n",
    "            pre_align_pose = align_pose * sapien.Pose([0, 0, -0.10])  # Go above before dropping\n",
    "\n",
    "            # execute motion sequence using IK solver\n",
    "            RobotArmMotionPlanningSolver = getMotionPlanner(env)\n",
    "            planner = RobotArmMotionPlanningSolver(\n",
    "                env,\n",
    "                debug=False,\n",
    "                vis=vis,\n",
    "                base_pose=env.unwrapped.agent.robot.pose,\n",
    "                visualize_target_grasp_pose=vis,\n",
    "                print_env_info=False,\n",
    "            )\n",
    "            planner.move_to_pose_with_screw(reach_pose)\n",
    "           \n",
    "            \n",
    "            #get the current observation from top\n",
    "            \n",
    "            obs1 = env.base_env.get_obs() \n",
    "            print(obs1[\"sensor_data\"].keys())\n",
    "            image_top = obs1[\"sensor_data\"][\"render_camera\"][\"rgb\"][0].clone().detach()\n",
    "            #for key in obs1[\"sensor_data\"][\"render_camera\"].keys():\n",
    "            #   print(key)\n",
    "\n",
    "            #image_top = image_top.cpu().numpy()\n",
    "            #plt.imshow(image_top)\n",
    "            #plt.axis('off')  # Turn off axis numbers/labels\n",
    "            #plt.show()\n",
    "    \n",
    "            if model:\n",
    "                _, _, _, token_pred = model.make_predictions(image_top, prefix)\n",
    "                json_dict[\"prediction\"] = token_pred\n",
    "                if token_pred == \"\" or token_pred is None:\n",
    "                    print(\"Warning: empty prediction, failing\")\n",
    "                    json_dict[\"reward\"] = 0\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    yield image_before, json_dict, args.seed[0]\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    curve_3d_pred, orns_3d_pred = dec_func(token_pred, camera=camera, robot_pose=robot_pose)\n",
    "                    curve_3d = curve_3d_pred  # set the unparsed trajectory one used for policy\n",
    "                    orns_3d = orns_3d_pred\n",
    "                # TODO(max): this should only catch value errors\n",
    "                except:\n",
    "                    print(\"Warning: exception during decoding tokens, failing\", token_pred)\n",
    "                    json_dict[\"reward\"] = 0\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    yield image_before, json_dict, args.seed[0]\n",
    "                    continue\n",
    "            # start and stop poses\n",
    "            if curve_3d.shape[1] != 2 or orns_3d.shape[1] != 2:\n",
    "                print(\"Warning: Model decoded something that is not a valid trajectory\")\n",
    "                json_dict[\"reward\"] = 0.0\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                yield image_before, json_dict, args.seed[0]\n",
    "                N_valid_samples += 1\n",
    "                continue\n",
    "\n",
    "            # convert two keypoints into motion sequence\n",
    "            _, curve_3d_i = generate_curve_torch(curve_3d[:, 0], curve_3d[:, -1], num_points=3)\n",
    "            print(f\"origin grasp pos:{grasp_pose}\")\n",
    "            grasp_pose_rotation = Pose.create_from_pq(p=curve_3d[:, 0], q=orns_3d[:, 0])\n",
    "            grasp_pose.raw_pose[0, 3:] = grasp_pose_rotation.raw_pose[0, 3:]\n",
    "            print(f\"Updated grasp pos:{grasp_pose}\")\n",
    "           \n",
    "\n",
    "            \n",
    "            planner.move_to_pose_with_screw(grasp_pose)\n",
    "            # run_interactive(env)\n",
    "            planner.close_gripper()\n",
    "            planner.move_to_pose_with_screw(lift_pose)\n",
    "            planner.move_to_pose_with_screw(pre_align_pose)\n",
    "            planner.move_to_pose_with_screw(align_pose)\n",
    "            # run_interactive(env)\n",
    "            planner.open_gripper()\n",
    "            final_reward = env.unwrapped.eval_reward()[0]\n",
    "            planner.close()\n",
    "    \n",
    "            json_dict[\"reward\"] = float(final_reward)\n",
    "            if verbose:\n",
    "                print(f\"reward {final_reward:0.2f} seed\", args.seed[0])\n",
    "\n",
    "        elif args.run_mode == \"interactive\":\n",
    "            run_interactive(env)\n",
    "        elif args.run_mode == \"first\":\n",
    "            # only render first frame\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        # adding the top view image to \n",
    "        if \"top\" in str(args.camera_views):\n",
    "            image_before = (image_top, image_before)\n",
    "            #obs[\"sensor_data\"][\"render_camera\"][\"depth\"][0] = obs1[\"sensor_data\"][\"hand_camera\"][\"rgb\"][0].clone().detach()\n",
    "\n",
    "        if args.record_dir:\n",
    "            # if i % SAVE_FREQ == 0:\n",
    "            # keep the transition from reset (which does not have an action)\n",
    "\n",
    "            downcast_seg_array(env)\n",
    "            env.flush_trajectory(save=True, ignore_empty_transition=False)\n",
    "            # to skip saving do: env.flush_trajectory(save=False)\n",
    "\n",
    "            if SAVE_VIDEO:\n",
    "                video_name = f\"CLEVR_{str(args.seed[0]).zfill(10)}\"\n",
    "                env.flush_video(name=video_name, save=True)\n",
    "\n",
    "        del obs\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "       \n",
    "            \n",
    "        yield image_before, json_dict, args.seed[0]\n",
    "\n",
    "        N_valid_samples += 1\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "def run_interactive(env):\n",
    "    env.print_sim_details()\n",
    "    print(\"Entering do nothing loop: Ctrl-C to continue\")\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(.1)\n",
    "            env.base_env.render_human()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nCtrl+C detected, continuing.\")\n",
    "\n",
    "\n",
    "def run_iteration(parsed_args, N_samples, process_num=None, progress_bar=None):\n",
    "    \"\"\"Runs the environment iteration in a separate process.\"\"\"\n",
    "    env_iter = iterate_env(parsed_args, vis=False)\n",
    "    for _ in range(N_samples):\n",
    "        _ = next(env_iter)\n",
    "        if progress_bar is not None:\n",
    "            progress_bar.value += 1\n",
    "\n",
    "\n",
    "def save_multiproces(parsed_args, N_samples, N_processes=10):\n",
    "    from mani_skill.examples.cvla.utils_record import check_no_uncommitted_changes, get_git_commit_hash\n",
    "    parsed_args.run_mode = \"first\"\n",
    "    dataset_path = Path(parsed_args.record_dir)\n",
    "    os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "    # save command line arguments in nice format\n",
    "    if N_samples > 100:\n",
    "        check_no_uncommitted_changes()\n",
    "    commit_hash = get_git_commit_hash()\n",
    "    with open(dataset_path / \"args.txt\", \"w\") as f:\n",
    "        f.write(f\"git_commit: {commit_hash}\\n\")\n",
    "        for arg in vars(parsed_args):\n",
    "            f.write(f\"{arg}: {getattr(parsed_args, arg)}\\n\")\n",
    "\n",
    "    # set random seeds, be careful to not copy same seeds between processes\n",
    "    if N_processes > 1:\n",
    "        assert parsed_args.seed is None\n",
    "    if isinstance(parsed_args.seed, int):\n",
    "        assert N_processes == 1\n",
    "        rng = np.random.default_rng(parsed_args.seed)\n",
    "        parsed_args.seed = rng.integers(0, RAND_MAX, N_samples).tolist()\n",
    "\n",
    "    # don't multiprocess\n",
    "    if N_processes == 1:\n",
    "        # don't set N_samples in iterate_env, so that e.g. re-generate can work for visibility\n",
    "        env_iter = iterate_env(parsed_args, vis=False)\n",
    "        for _ in tqdm(range(N_samples)):\n",
    "            try:\n",
    "                _ = next(env_iter)\n",
    "            except StopIteration:\n",
    "                break\n",
    "    else:\n",
    "        samples_per_process = N_samples // N_processes\n",
    "        progress_bar = multiprocessing.Value(\"i\", 0)\n",
    "\n",
    "        tasks = []\n",
    "        for p_num in range(N_processes):\n",
    "            dataset_path_p = Path(dataset_path) / f\"p{p_num}\"\n",
    "            os.makedirs(dataset_path_p, exist_ok=True)\n",
    "            args_copy = deepcopy(parsed_args)\n",
    "            args_copy.record_dir = dataset_path_p\n",
    "            p = multiprocessing.Process(target=run_iteration, args=(args_copy, samples_per_process, p_num, progress_bar), name=f\"Worker-{p_num+1}\")\n",
    "            tasks.append(p)\n",
    "            p.start()\n",
    "            time.sleep(1.1)  # Give some time for processes to start\n",
    "\n",
    "        # Display tqdm progress in the main process\n",
    "        with tqdm(total=N_samples, desc=\"Total Progress\", position=0, leave=True) as pbar:\n",
    "            last_count = 0\n",
    "            while any(p.is_alive() for p in tasks):  # Update while processes are running\n",
    "                current_count = progress_bar.value\n",
    "                pbar.update(current_count - last_count)  # Update tqdm only for new progress\n",
    "                last_count = current_count\n",
    "                time.sleep(1)  # Prevents excessive updates\n",
    "\n",
    "        # await asyncio.gather(*tasks)\n",
    "        for p in tasks:\n",
    "            p.join()  # Wait for all processes to finish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d553c44-265d-43cc-bb4d-129c3e42ada2",
   "metadata": {},
   "source": [
    "### Generate the training data in simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b30fcbc1-3eba-4b2c-a831-e82a3f87d785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "488ffcd4-946b-4d18-8018-e5e55daea278",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                              | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pose\n",
      "Pose(raw_pose=tensor([[0.0000, 0.0000, 0.6000, 0.5000, 0.0000, 0.5000, 0.0000]]))\n",
      "!!!\n",
      "pose\n",
      "Pose(raw_pose=tensor([[-0.0173,  0.0368,  1.0000,  0.5000,  0.0000,  0.5000,  0.0000]]))\n",
      "action encoder xyzrotvec-cam-1024xy\n",
      "filter visible objects True\n",
      "!!!\n",
      "pose\n",
      "Pose(raw_pose=tensor([[-0.1083,  0.0564,  1.0000,  0.5000,  0.0000,  0.5000,  0.0000]]))\n",
      "obj_st\n",
      "Pose(raw_pose=tensor([[-0.0536,  0.0898,  0.0000,  0.9931,  0.0000,  0.0000,  0.1175]]))\n",
      "grasp_pose\n",
      "Pose(raw_pose=tensor([[-0.0453,  0.0916,  0.1075,  0.0000,  0.9927,  0.1207,  0.0000]]))\n",
      "dict_keys(['render_camera', 'hand_camera'])\n",
      "origin grasp pos:Pose(raw_pose=tensor([[-4.5106e-02,  9.1109e-02,  1.1000e-01,  3.5059e-09,  9.9298e-01,\n",
      "          1.1729e-01,  1.5209e-02]]))\n",
      "Updated grasp pos:Pose(raw_pose=tensor([[-4.5106e-02,  9.1109e-02,  1.1000e-01,  3.5059e-09,  9.9298e-01,\n",
      "          1.1729e-01,  1.5209e-02]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▋                                                                                | 1/15 [00:20<04:42, 20.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!\n",
      "pose\n",
      "Pose(raw_pose=tensor([[-0.1551, -0.0074,  1.0000,  0.5000,  0.0000,  0.5000,  0.0000]]))\n",
      "obj_st\n",
      "Pose(raw_pose=tensor([[-0.1243,  0.1621,  0.0000,  0.5677,  0.0000,  0.0000,  0.8232]]))\n",
      "grasp_pose\n",
      "Pose(raw_pose=tensor([[-0.1243,  0.1621,  0.0750,  0.0000,  0.5677,  0.8232,  0.0000]]))\n",
      "dict_keys(['render_camera', 'hand_camera'])\n",
      "origin grasp pos:Pose(raw_pose=tensor([[-1.2347e-01,  1.6073e-01,  8.0000e-02,  2.4482e-08,  5.7474e-01,\n",
      "          8.1825e-01, -1.2327e-02]]))\n",
      "Updated grasp pos:Pose(raw_pose=tensor([[-1.2347e-01,  1.6073e-01,  8.0000e-02,  2.4482e-08,  5.7474e-01,\n",
      "          8.1825e-01, -1.2327e-02]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|███████████▍                                                                          | 2/15 [00:31<03:12, 14.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!\n",
      "pose\n",
      "Pose(raw_pose=tensor([[-0.2051,  0.1416,  1.0000,  0.5000,  0.0000,  0.5000,  0.0000]]))\n",
      "obj_st\n",
      "Pose(raw_pose=tensor([[-0.1419, -0.1811,  0.0000,  0.4019, -0.0000, -0.0000, -0.9157]]))\n",
      "grasp_pose\n",
      "Pose(raw_pose=tensor([[-0.1429, -0.1795,  0.0165,  0.0000,  0.8667,  0.4989,  0.0000]]))\n",
      "dict_keys(['render_camera', 'hand_camera'])\n",
      "origin grasp pos:Pose(raw_pose=tensor([[-1.4238e-01, -1.7828e-01,  2.0000e-02,  1.4709e-08,  8.7101e-01,\n",
      "          4.9109e-01,  1.2972e-02]]))\n",
      "Updated grasp pos:Pose(raw_pose=tensor([[-1.4238e-01, -1.7828e-01,  2.0000e-02,  1.4709e-08,  8.7101e-01,\n",
      "          4.9109e-01,  1.2972e-02]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████▏                                                                    | 3/15 [00:45<02:57, 14.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!\n",
      "pose\n",
      "Pose(raw_pose=tensor([[-0.1600,  0.0771,  1.0000,  0.5000,  0.0000,  0.5000,  0.0000]]))\n",
      "obj_st\n",
      "Pose(raw_pose=tensor([[-0.1600,  0.0771,  0.0000,  0.9010,  0.0000,  0.0000,  0.4338]]))\n",
      "grasp_pose\n",
      "Pose(raw_pose=tensor([[-0.1600,  0.0771,  0.0750,  0.0000,  0.9010,  0.4338,  0.0000]]))\n",
      "dict_keys(['render_camera', 'hand_camera'])\n",
      "origin grasp pos:Pose(raw_pose=tensor([[-1.5930e-01,  7.6880e-02,  8.0000e-02,  1.2836e-08,  9.0502e-01,\n",
      "          4.2515e-01,  1.3436e-02]]))\n",
      "Updated grasp pos:Pose(raw_pose=tensor([[-1.5930e-01,  7.6880e-02,  8.0000e-02,  1.2836e-08,  9.0502e-01,\n",
      "          4.2515e-01,  1.3436e-02]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██████████████████████▉                                                               | 4/15 [00:58<02:34, 14.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!\n",
      "pose\n",
      "Pose(raw_pose=tensor([[0.1346, 0.1155, 1.0000, 0.5000, 0.0000, 0.5000, 0.0000]]))\n",
      "obj_st\n",
      "Pose(raw_pose=tensor([[-0.1065, -0.0404,  0.0000,  0.6824,  0.0000,  0.0000,  0.7310]]))\n",
      "grasp_pose\n",
      "Pose(raw_pose=tensor([[-0.1013, -0.0404,  0.0175,  0.0000,  0.7395,  0.6732,  0.0000]]))\n",
      "dict_keys(['render_camera', 'hand_camera'])\n",
      "origin grasp pos:Pose(raw_pose=tensor([[-1.0129e-01, -4.0138e-02,  2.0000e-02,  1.1483e-02, -7.4110e-01,\n",
      "         -6.7130e-01,  3.3419e-08]]))\n",
      "Updated grasp pos:Pose(raw_pose=tensor([[-1.0129e-01, -4.0138e-02,  2.0000e-02,  1.1483e-02, -7.4110e-01,\n",
      "         -6.7130e-01,  3.3419e-08]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████▋                                                         | 5/15 [01:14<02:24, 14.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!\n",
      "pose\n",
      "Pose(raw_pose=tensor([[-0.2293, -0.1174,  1.0000,  0.5000,  0.0000,  0.5000,  0.0000]]))\n",
      "obj_st\n",
      "Pose(raw_pose=tensor([[-0.2293, -0.1174,  0.0000,  0.9207,  0.0000,  0.0000,  0.3902]]))\n",
      "grasp_pose\n",
      "Pose(raw_pose=tensor([[-0.2292, -0.1173,  0.0750,  0.0000,  0.5485,  0.8362,  0.0000]]))\n",
      "dict_keys(['render_camera', 'hand_camera'])\n",
      "origin grasp pos:Pose(raw_pose=tensor([[-1.6214e-01, -1.1518e-01,  8.0000e-02,  2.4852e-08,  5.5432e-01,\n",
      "          8.3221e-01, -1.2491e-02]]))\n",
      "Updated grasp pos:Pose(raw_pose=tensor([[-1.6214e-01, -1.1518e-01,  8.0000e-02,  2.4852e-08,  5.5432e-01,\n",
      "          8.3221e-01, -1.2491e-02]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████▍                                                   | 6/15 [01:24<01:59, 13.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!\n",
      "pose\n",
      "Pose(raw_pose=tensor([[0.1072, 0.1256, 1.0000, 0.5000, 0.0000, 0.5000, 0.0000]]))\n",
      "obj_st\n",
      "Pose(raw_pose=tensor([[0.1072, 0.1256, 0.0000, 0.9933, 0.0000, 0.0000, 0.1158]]))\n",
      "grasp_pose\n",
      "Pose(raw_pose=tensor([[0.1049, 0.1265, 0.0316, 0.0000, 0.8861, 0.4635, 0.0000]]))\n",
      "dict_keys(['render_camera', 'hand_camera'])\n",
      "origin grasp pos:Pose(raw_pose=tensor([[1.0547e-01, 1.2730e-01, 3.0000e-02, 1.4097e-08, 8.8289e-01, 4.6939e-01,\n",
      "         1.3129e-02]]))\n",
      "Updated grasp pos:Pose(raw_pose=tensor([[1.0547e-01, 1.2730e-01, 3.0000e-02, 1.4097e-08, 8.8289e-01, 4.6939e-01,\n",
      "         1.3129e-02]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████████████████████████████████████████▏                                             | 7/15 [01:43<01:59, 14.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!\n",
      "pose\n",
      "Pose(raw_pose=tensor([[0.1058, 0.0958, 1.0000, 0.5000, 0.0000, 0.5000, 0.0000]]))\n",
      "obj_st\n",
      "Pose(raw_pose=tensor([[-0.1552,  0.0129,  0.0000,  0.9879,  0.0000,  0.0000,  0.1551]]))\n",
      "grasp_pose\n",
      "Pose(raw_pose=tensor([[-0.1556,  0.0129,  0.0874,  0.0000,  0.8534,  0.5213,  0.0000]]))\n",
      "dict_keys(['render_camera', 'hand_camera'])\n",
      "origin grasp pos:Pose(raw_pose=tensor([[-1.5417e-01,  1.3083e-02,  9.0000e-02,  1.5308e-08,  8.5860e-01,\n",
      "          5.1248e-01,  1.2814e-02]]))\n",
      "Updated grasp pos:Pose(raw_pose=tensor([[-1.5417e-01,  1.3083e-02,  9.0000e-02,  1.5308e-08,  8.5860e-01,\n",
      "          5.1248e-01,  1.2814e-02]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████████████████████████████████████████████▊                                        | 8/15 [01:56<01:39, 14.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!\n",
      "pose\n",
      "Pose(raw_pose=tensor([[-0.2745,  0.0146,  1.0000,  0.5000,  0.0000,  0.5000,  0.0000]]))\n",
      "obj_st\n",
      "Pose(raw_pose=tensor([[-0.1053, -0.0125,  0.0000,  0.2418, -0.0000, -0.0000, -0.9703]]))\n",
      "grasp_pose\n",
      "Pose(raw_pose=tensor([[-0.1053, -0.0132,  0.0304,  0.0000,  0.5246,  0.8514,  0.0000]]))\n",
      "dict_keys(['render_camera', 'hand_camera'])\n",
      "origin grasp pos:Pose(raw_pose=tensor([[-1.0549e-01, -1.3078e-02,  3.0000e-02,  2.5209e-08,  5.3357e-01,\n",
      "          8.4566e-01, -1.2653e-02]]))\n",
      "Updated grasp pos:Pose(raw_pose=tensor([[-1.0549e-01, -1.3078e-02,  3.0000e-02,  2.5209e-08,  5.3357e-01,\n",
      "          8.4566e-01, -1.2653e-02]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████▌                                  | 9/15 [02:07<01:19, 13.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!\n",
      "pose\n",
      "Pose(raw_pose=tensor([[-0.1643, -0.2132,  1.0000,  0.5000,  0.0000,  0.5000,  0.0000]]))\n",
      "obj_st\n",
      "Pose(raw_pose=tensor([[-0.1008,  0.2083,  0.0000,  0.6544,  0.0000,  0.0000,  0.7561]]))\n",
      "grasp_pose\n",
      "Pose(raw_pose=tensor([[-0.1105,  0.2028,  0.0406,  0.0000,  0.7880,  0.6157,  0.0000]]))\n",
      "dict_keys(['render_camera', 'hand_camera'])\n",
      "origin grasp pos:Pose(raw_pose=tensor([[-1.1046e-01,  2.0308e-01,  4.0000e-02,  1.8117e-08,  7.8882e-01,\n",
      "          6.1450e-01,  1.1994e-02]]))\n",
      "Updated grasp pos:Pose(raw_pose=tensor([[-1.1046e-01,  2.0308e-01,  4.0000e-02,  1.8117e-08,  7.8882e-01,\n",
      "          6.1450e-01,  1.1994e-02]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████████████████████████████████████▋                            | 10/15 [02:23<01:10, 14.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pose\n",
      "Pose(raw_pose=tensor([[0.0000, 0.0000, 0.6000, 0.5000, 0.0000, 0.5000, 0.0000]]))\n",
      "!!!\n",
      "pose\n",
      "Pose(raw_pose=tensor([[-0.1169, -0.0738,  1.0000,  0.5000,  0.0000,  0.5000,  0.0000]]))\n",
      "!!!\n",
      "pose\n",
      "Pose(raw_pose=tensor([[-0.0172,  0.0980,  1.0000,  0.5000,  0.0000,  0.5000,  0.0000]]))\n",
      "obj_st\n",
      "Pose(raw_pose=tensor([[-0.3309, -0.0553,  0.0000,  0.5645,  0.0000,  0.0000,  0.8254]]))\n",
      "grasp_pose\n",
      "Pose(raw_pose=tensor([[-0.3305, -0.0552,  0.0312,  0.0000,  0.8374,  0.5466,  0.0000]]))\n",
      "dict_keys(['render_camera', 'hand_camera'])\n",
      "origin grasp pos:Pose(raw_pose=tensor([[-3.1523e-01, -5.5238e-02,  3.0000e-02,  1.6470e-08,  8.3221e-01,\n",
      "          5.5432e-01,  1.2491e-02]]))\n",
      "Updated grasp pos:Pose(raw_pose=tensor([[-3.1523e-01, -5.5238e-02,  3.0000e-02,  1.6470e-08,  8.3221e-01,\n",
      "          5.5432e-01,  1.2491e-02]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|██████████████████████████████████████████████████████████████▎                      | 11/15 [02:46<01:07, 16.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!\n",
      "pose\n",
      "Pose(raw_pose=tensor([[ 0.1574, -0.0379,  1.0000,  0.5000,  0.0000,  0.5000,  0.0000]]))\n",
      "obj_st\n",
      "Pose(raw_pose=tensor([[-0.3528, -0.0889,  0.0000,  0.8053,  0.0000,  0.0000, -0.5929]]))\n",
      "grasp_pose\n",
      "Pose(raw_pose=tensor([[-0.3528, -0.0889,  0.0350,  0.0000,  0.9887,  0.1502,  0.0000]]))\n",
      "dict_keys(['render_camera', 'hand_camera'])\n",
      "origin grasp pos:Pose(raw_pose=tensor([[-2.5988e-01, -8.7319e-02,  4.0000e-02,  4.2893e-09,  9.8978e-01,\n",
      "          1.4182e-01,  1.5088e-02]]))\n",
      "Updated grasp pos:Pose(raw_pose=tensor([[-2.5988e-01, -8.7319e-02,  4.0000e-02,  4.2893e-09,  9.8978e-01,\n",
      "          1.4182e-01,  1.5088e-02]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████████████████████████████                 | 12/15 [03:05<00:52, 17.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!\n",
      "pose\n",
      "Pose(raw_pose=tensor([[0.1041, 0.1284, 1.0000, 0.5000, 0.0000, 0.5000, 0.0000]]))\n",
      "obj_st\n",
      "Pose(raw_pose=tensor([[ 0.1041,  0.1284,  0.0000,  0.8495,  0.0000,  0.0000, -0.5276]]))\n",
      "grasp_pose\n",
      "Pose(raw_pose=tensor([[0.1042, 0.1284, 0.0085, 0.0000, 0.5354, 0.8446, 0.0000]]))\n",
      "dict_keys(['render_camera', 'hand_camera'])\n",
      "origin grasp pos:Pose(raw_pose=tensor([[ 1.0410e-01,  1.2783e-01,  1.0000e-02,  2.5209e-08,  5.3357e-01,\n",
      "          8.4566e-01, -1.2653e-02]]))\n",
      "Updated grasp pos:Pose(raw_pose=tensor([[ 1.0410e-01,  1.2783e-01,  1.0000e-02,  2.5209e-08,  5.3357e-01,\n",
      "          8.4566e-01, -1.2653e-02]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|█████████████████████████████████████████████████████████████████████████▋           | 13/15 [03:20<00:33, 16.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!\n",
      "pose\n",
      "Pose(raw_pose=tensor([[ 0.0955, -0.0080,  1.0000,  0.5000,  0.0000,  0.5000,  0.0000]]))\n",
      "obj_st\n",
      "Pose(raw_pose=tensor([[-0.1883,  0.1204,  0.0000,  0.3338, -0.0000, -0.0000, -0.9427]]))\n",
      "grasp_pose\n",
      "Pose(raw_pose=tensor([[-0.1881,  0.1203,  0.0024,  0.0000,  0.5443,  0.8389,  0.0000]]))\n",
      "dict_keys(['render_camera', 'hand_camera'])\n",
      "origin grasp pos:Pose(raw_pose=tensor([[-1.8907e-01,  1.2053e-01,  0.0000e+00,  2.4852e-08,  5.5432e-01,\n",
      "          8.3221e-01, -1.2491e-02]]))\n",
      "Updated grasp pos:Pose(raw_pose=tensor([[-1.8907e-01,  1.2053e-01,  0.0000e+00,  2.4852e-08,  5.5432e-01,\n",
      "          8.3221e-01, -1.2491e-02]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|███████████████████████████████████████████████████████████████████████████████▎     | 14/15 [03:33<00:15, 15.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!\n",
      "pose\n",
      "Pose(raw_pose=tensor([[ 0.0741, -0.0061,  1.0000,  0.5000,  0.0000,  0.5000,  0.0000]]))\n",
      "obj_st\n",
      "Pose(raw_pose=tensor([[ 0.1910, -0.1248,  0.0000,  0.4784,  0.0000,  0.0000,  0.8781]]))\n",
      "grasp_pose\n",
      "Pose(raw_pose=tensor([[ 0.1907, -0.1201,  0.0932,  0.0000,  0.4741,  0.8805,  0.0000]]))\n",
      "dict_keys(['render_camera', 'hand_camera'])\n",
      "origin grasp pos:Pose(raw_pose=tensor([[ 1.9202e-01, -1.2116e-01,  9.0000e-02,  2.6204e-08,  4.6939e-01,\n",
      "          8.8289e-01, -1.3129e-02]]))\n",
      "Updated grasp pos:Pose(raw_pose=tensor([[ 1.9202e-01, -1.2116e-01,  9.0000e-02,  2.6204e-08,  4.6939e-01,\n",
      "          8.8289e-01, -1.3129e-02]]))\n",
      "screw plan failed\n",
      "screw plan failed\n",
      "screw plan failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 15/15 [04:37<00:00, 18.50s/it]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# For batch data collection without visualization\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "args = Args(\n",
    "    robot_uids=\"panda_wristcam\",\n",
    "    record_dir=\"/work/dlclarge2/zhangj-zhangj-CFM/data/p6\",\n",
    "    N_samples=15,                # Number of samples to generate\n",
    "    camera_views=\"top\",\n",
    "    object_dataset=\"objaverse\",\n",
    "    shader=\"default\",\n",
    "    obs_mode=\"rgb+segmentation+depth\",\n",
    "    render_mode=\"rgb_array\",              # Disable visualization for speed\n",
    "    num_envs=1,\n",
    "    run_mode = \"script\",\n",
    "    quiet = True\n",
    ")\n",
    "\n",
    "inital_seed = 2919129908\n",
    "random.seed(inital_seed)\n",
    "seeds = random.sample(range(0, 2**32), 15)\n",
    "#args.seed = seeds\n",
    "env_iter = iterate_env(args, vis=False)\n",
    "for _ in tqdm(range(args.N_samples)):\n",
    "            try:\n",
    "                _ = next(env_iter)\n",
    "            except StopIteration:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191388e8-6c33-4561-8cef-8830997ded39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[[175045574], [3113457551], [2542170577], [3893435155], [4207407346], [4264475535], [37484697], [3489624216], [3939266399], [3477784267], [2998306144], [1524371290], [1411403658], [3730714662], [3253628556]]\n",
    "#[2603274749, 753606222, 24081061, 2040171868, 1774913893, 2908100739, 172158870, \n",
    "            #461653925, 2280014332, 3431745312, 433261562, 2068926005, 1277534805, 2311723683, 4033777556]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ffddc6-e199-4183-a069-d71bb78d4a11",
   "metadata": {},
   "source": [
    "### Evaluate on the simulation evn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1993017-7099-4c4d-bdf6-92e8257d2050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73045b6e-eb02-4ba4-98a5-32cd1a6ec564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "#v17 = \"cvla-clevr-camRF-sceneR-9__img_1_pr_interleaved_enc_xyzrotvec-cam-512xy128dmaxTokens13_lr3e-05_sortAll_augs_max20k_2025-04-27_18-00-52\" # 6 - 20\n",
    "#model_path = Path(\"/work/dlclarge2/bratulic-cvla/models/\") / v17 / \"checkpoint-19000\"\n",
    "model_path = Path(\"/data/lmbraid19/argusm/models/_text_lr3e-05xyzrotvec-cam-512xy256d_2025-04-23_12-03-48\")/\"checkpoint-4687\"\n",
    "#model_path = Path(\"/work/dlclarge2/bratulic-cvla/models\")  / \"_text_lr3e-05_enc512_128d_depth_2025-04-29_10-38-15\" / \"checkpoint-4687\"\n",
    "model = cVLA_wrapped(model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b89c8e-7fd5-45c5-85cf-729d281c978b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# For batch data collection without visualization\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "args = Args(\n",
    "    robot_uids=\"panda_wristcam\",\n",
    "    record_dir=\"/work/dlclarge2/zhangj-zhangj-CFM/data\",\n",
    "    N_samples=20,                # Number of samples to generate\n",
    "    object_dataset=\"objaverse\",\n",
    "    shader=\"default\",\n",
    "    obs_mode=\"rgb+segmentation\",\n",
    "    render_mode=\"rgb_array\",              # Disable visualization for speed\n",
    "    num_envs=1,\n",
    "    run_mode = \"script\",\n",
    "    quiet = True,\n",
    "    action_encoder= model.enc_model.NAME\n",
    ")\n",
    "\n",
    "inital_seed = 2919129908\n",
    "random.seed(inital_seed)\n",
    "seeds = random.sample(range(0, 2**32), args.N_samples)\n",
    "#args.seed = seeds\n",
    "\n",
    "env_iter = iterate_env(args, vis=False, model=model)\n",
    "for _ in tqdm(range(args.N_samples)):\n",
    "            try:\n",
    "                _ = next(env_iter)\n",
    "            except StopIteration:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfae5a0-a142-4b29-ac88-7bb2825f30f3",
   "metadata": {},
   "source": [
    "### Find the largest h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb32718-c7df-4010-bb22-110749da21d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "video_dir = \"/work/dlclarge2/zhangj-zhangj-CFM/data/p_ran\"\n",
    "\n",
    "# Get list of .h5 files with their sizes\n",
    "h5_files = [\n",
    "    (f, os.path.getsize(os.path.join(video_dir, f)))\n",
    "    for f in os.listdir(video_dir)\n",
    "    if f.endswith('.h5')\n",
    "]\n",
    "print(h5_files)\n",
    "\n",
    "# Select the file with the maximum size\n",
    "if h5_files:\n",
    "    largest_file = max(h5_files, key=lambda x: x[1])[0]\n",
    "    print(f\"Largest .h5 file: {largest_file}\")\n",
    "else:\n",
    "    print(\"No .h5 files found.\")\n",
    "#Largest .h5 file: 20250522_155344.h5//20250522_164107.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b63e34-24fe-46ab-8827-ebf546f7b8c9",
   "metadata": {},
   "source": [
    "### Generate the videos based on certain .h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4162cc93-418f-4462-b1dc-e58be617bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from cvla.data_loader_h5 import H5Dataset\n",
    "video_dir = \"/work/dlclarge2/zhangj-zhangj-CFM/data\"\n",
    "dataset_location = Path(video_dir) / \"p_ran\"/\"20250708_081652.h5\" # specify the h5 file\n",
    "#\"20250523_182213.h5\"  \"20250523_181858.h5\"\n",
    "#\"20250523_175807.h5\" #20250523_175522.h5\" #20250523_174047.h5\" \n",
    "#20250523_172840.h5\"\n",
    "dataset = H5Dataset(dataset_location)\n",
    "video_dir = Path(video_dir)/ \"video\"\n",
    "output_path = video_dir / \"run_0.mp4\"\n",
    "\n",
    "for key in tqdm(sorted(dataset.h5_file.keys())):\n",
    "    key2 = int(key[5]) + 13\n",
    "    key2 = str(\"traj_\") + str(key2)\n",
    "    \n",
    "    frames = dataset.h5_file[f\"{key}/obs/sensor_data/render_camera/rgb\"]\n",
    "\n",
    "    # Define video parameters\n",
    "    height, width = frames.shape[1:3]\n",
    "    fps = 30  # or whatever frame rate you want\n",
    "    output_path = video_dir / f'video_523_20_top_{key2}.mp4'\n",
    "\n",
    "    # Define the video writer using MP4 codec\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # or 'avc1'\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Write each frame\n",
    "    for frame in frames:\n",
    "        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # OpenCV expects BGR\n",
    "        out.write(frame_bgr)\n",
    "\n",
    "    out.release()\n",
    "    print(f\"Video saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1f8c37-6ade-4e7e-8798-cdd97dc71261",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(Path(args.record_dir).rglob(\"*.mp4\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0589d6-ebae-40ca-bc65-46531fb5e207",
   "metadata": {},
   "source": [
    "### save the images of top view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70745a31-fa42-4f05-83d6-16c932e94fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "save_dir = \"/work/dlclarge2/zhangj-zhangj-CFM/data/top_images\"\n",
    "\n",
    "for i, image_tensor in enumerate(top_view):\n",
    "\n",
    "    image_np = image_tensor.cpu().numpy()\n",
    "\n",
    "    if image_np.shape[0] == 3:  \n",
    "        image_np = image_np.transpose(1, 2, 0)\n",
    "  \n",
    "    save_path = os.path.join(save_dir, f\"top_view_{i}.png\")  \n",
    "    plt.imsave(save_path, image_np)  \n",
    "    print(f\"Saved: {save_path}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84fa913-d805-4ce9-a717-0c93306fefb4",
   "metadata": {},
   "source": [
    "### In time visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ee2b8d-82f3-4542-935f-375eb53820e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict\n",
    "import json\n",
    "\n",
    "# 1. Create an Args object with your desired parameters\n",
    "args = Args(\n",
    "    env_id=\"CvlaMove-v1\",\n",
    "    record_dir=\"/work/dlclarge2/zhangj-zhangj-CFM/data\",  # Required for saving\n",
    "    N_samples=50,                    # Number of samples to generate\n",
    "    object_dataset=\"objaverse\",          # or \"ycb\" or \"objaverse\"\n",
    "    shader=\"default\",                # or \"rt\" for ray tracing\n",
    "    obs_mode=\"rgb+depth+segmentation\",\n",
    "    control_mode=\"pd_joint_pos\",\n",
    "    # Add other parameters as needed\n",
    ")\n",
    "\n",
    "# 2. For visualization (optional)\n",
    "args.render_mode = \"human\"  # Enable visualization\n",
    "args.num_envs = 1           # For visualization, keep at 1\n",
    "\n",
    "# 3. Run the environment iteration\n",
    "env_iter = iterate_env(args, vis=True)\n",
    "\n",
    "# 4. To run a specific number of episodes:\n",
    "for _ in range(5):  # Run 5 episodes\n",
    "    try:\n",
    "        image_before, json_dict, seed = next(env_iter)\n",
    "        # You can process the outputs here\n",
    "        print(f\"Completed episode with seed {seed}\")\n",
    "    except StopIteration:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87cdd0e-f1b0-4f4f-a12b-b77047064073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
